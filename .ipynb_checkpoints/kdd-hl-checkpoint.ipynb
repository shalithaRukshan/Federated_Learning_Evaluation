{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#6B49F5> A Simple Implementation of FedAvg with PyTorch on IIDÂ Data </font> \n",
    "Please see https://towardsdatascience.com/federated-learning-a-simple-implementation-of-fedavg-federated-averaging-with-pytorch-90187c9c9577 for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pd.options.display.float_format = \"{:,.4f}\".format\n",
    "sm = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "THREAT_TYPE = 'threat_type'\n",
    "THREAT_HL = 'threat_hl'\n",
    "\n",
    "learning_rate = 0.01\n",
    "numEpoch = 10\n",
    "batch_size = 32\n",
    "momentum = 0.9\n",
    "print_amount=3\n",
    "number_of_slices = 10\n",
    "\n",
    "data_path = \"D:\\\\learning\\\\PyTorch\\\\NSL_KDD-master\\\\\"\n",
    "\n",
    "colnames = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n",
    "            'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
    "            'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files',\n",
    "            'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
    "            'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "            'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "            'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "            'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "            'dst_host_srv_rerror_rate', 'threat_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack types in full set high level: \n",
      " 0    77053\n",
      "1    71463\n",
      "Name: threat_type, dtype: int64\n",
      "(148516, 42)\n",
      "Attack types in full set: \n",
      " 0    77053\n",
      "1    53387\n",
      "4    14077\n",
      "3     3880\n",
      "2      119\n",
      "Name: threat_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(data_path + \"KDDTrain+.csv\", header = None)\n",
    "df_train = df_train.iloc[:, :-1]\n",
    "\n",
    "df_test = pd.read_csv(data_path + \"KDDTest+.csv\", header = None)\n",
    "df_test = df_test.iloc[:, :-1]\n",
    "\n",
    "df_train.columns = colnames\n",
    "df_test.columns = colnames\n",
    "\n",
    "df_train.loc[(df_train['threat_type'] == 'back'), 'threat_type'] = 1\n",
    "df_train.loc[(df_train['threat_type'] == 'buffer_overflow'), 'threat_type'] = 2\n",
    "df_train.loc[(df_train['threat_type'] == 'ftp_write'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'guess_passwd'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'imap'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'ipsweep'), 'threat_type'] = 4\n",
    "df_train.loc[(df_train['threat_type'] == 'land'), 'threat_type'] = 1\n",
    "df_train.loc[(df_train['threat_type'] == 'loadmodule'), 'threat_type'] = 2\n",
    "df_train.loc[(df_train['threat_type'] == 'multihop'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'neptune'), 'threat_type'] = 1\n",
    "df_train.loc[(df_train['threat_type'] == 'nmap'), 'threat_type'] = 4\n",
    "df_train.loc[(df_train['threat_type'] == 'perl'), 'threat_type'] = 2\n",
    "df_train.loc[(df_train['threat_type'] == 'phf'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'pod'), 'threat_type'] = 1\n",
    "df_train.loc[(df_train['threat_type'] == 'portsweep'), 'threat_type'] = 4\n",
    "df_train.loc[(df_train['threat_type'] == 'rootkit'), 'threat_type'] = 2\n",
    "df_train.loc[(df_train['threat_type'] == 'satan'), 'threat_type'] = 4\n",
    "df_train.loc[(df_train['threat_type'] == 'smurf'), 'threat_type'] = 1\n",
    "df_train.loc[(df_train['threat_type'] == 'spy'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'teardrop'), 'threat_type'] = 1\n",
    "df_train.loc[(df_train['threat_type'] == 'warezclient'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'warezmaster'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'normal'), 'threat_type'] = 0\n",
    "df_train.loc[(df_train['threat_type'] == 'unknown'), 'threat_type'] = 6\n",
    "\n",
    "df_test.loc[(df_test['threat_type'] == 'back'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'buffer_overflow'), 'threat_type'] = 2\n",
    "df_test.loc[(df_test['threat_type'] == 'ftp_write'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'guess_passwd'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'imap'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'ipsweep'), 'threat_type'] = 4\n",
    "df_test.loc[(df_test['threat_type'] == 'land'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'loadmodule'), 'threat_type'] = 2\n",
    "df_test.loc[(df_test['threat_type'] == 'multihop'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'neptune'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'nmap'), 'threat_type'] = 4\n",
    "df_test.loc[(df_test['threat_type'] == 'perl'), 'threat_type'] = 2\n",
    "df_test.loc[(df_test['threat_type'] == 'phf'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'pod'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'portsweep'), 'threat_type'] = 4\n",
    "df_test.loc[(df_test['threat_type'] == 'rootkit'), 'threat_type'] = 2\n",
    "df_test.loc[(df_test['threat_type'] == 'satan'), 'threat_type'] = 4\n",
    "df_test.loc[(df_test['threat_type'] == 'smurf'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'spy'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'teardrop'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'warezclient'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'warezmaster'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'normal'), 'threat_type'] = 0\n",
    "df_test.loc[(df_test['threat_type'] == 'unknown'), 'threat_type'] = 6\n",
    "df_test.loc[(df_test['threat_type'] == 'mscan'), 'threat_type'] = 4\n",
    "df_test.loc[(df_test['threat_type'] == 'apache2'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'snmpgetattack'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'processtable'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'httptunnel'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'ps'), 'threat_type'] = 2\n",
    "df_test.loc[(df_test['threat_type'] == 'snmpguess'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'mailbomb'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'named'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'sendmail'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'xterm'), 'threat_type'] = 2\n",
    "df_test.loc[(df_test['threat_type'] == 'xlock'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'xsnoop'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'sqlattack'), 'threat_type'] = 2\n",
    "df_test.loc[(df_test['threat_type'] == 'udpstorm'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'saint'), 'threat_type'] = 4\n",
    "df_test.loc[(df_test['threat_type'] == 'worm'), 'threat_type'] = 1\n",
    "\n",
    "df_full = pd.concat([df_train, df_test])\n",
    "df_full_hl = copy.deepcopy(df_full)\n",
    "\n",
    "df_full_hl.loc[(df_full_hl[THREAT_TYPE] != 0), THREAT_TYPE] = 1\n",
    "print('Attack types in full set high level: \\n', df_full_hl[THREAT_TYPE].value_counts())\n",
    "print(df_full_hl.shape)\n",
    "\n",
    "print('Attack types in full set: \\n', df_full[THREAT_TYPE].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization shape of data set :  (148516, 42)\n",
      "(148516, 34)\n",
      "(148516, 33)\n",
      "After normalization shape of data set:  (148516, 34)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Before normalization shape of data set : ', df_full.shape)\n",
    "threat_type_df = df_full['threat_type'].copy()\n",
    "# Considering numerical columns\n",
    "# 34 numerical columns are considered for training\n",
    "numerical_colmanes = ['duration', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'urgent', 'hot',\n",
    "                      'num_failed_logins', 'num_compromised', 'root_shell', 'su_attempted', 'num_root',\n",
    "                      'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'count',\n",
    "                      'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "                      'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
    "                      'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "                      'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "                      'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
    "\n",
    "numerical_df_full = df_full[numerical_colmanes].copy()\n",
    "print(numerical_df_full.shape)\n",
    "#\n",
    "# # Lets remove the numerical columns with constant value\n",
    "numerical_df_full = numerical_df_full.loc[:, (numerical_df_full != numerical_df_full.iloc[0]).any()]\n",
    "#\n",
    "# # lets scale the values for each column from [0,1]\n",
    "# # N.B. we dont have any negative values]\n",
    "final_df_full = numerical_df_full / numerical_df_full.max()\n",
    "print(final_df_full.shape)\n",
    "\n",
    "df_normalized = pd.concat([final_df_full, threat_type_df], axis=1)\n",
    "print('After normalization shape of data set: ', df_normalized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization shape of data set :  (148516, 42)\n",
      "(148516, 34)\n",
      "(148516, 33)\n",
      "After normalization shape of data set:  (148516, 34)\n",
      "0    77053\n",
      "1    71463\n",
      "Name: threat_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Before normalization shape of data set : ', df_full_hl.shape)\n",
    "threat_type_df_hl = df_full_hl['threat_type'].copy()\n",
    "# Considering numerical columns\n",
    "# 34 numerical columns are considered for training\n",
    "numerical_colmanes = ['duration', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'urgent', 'hot',\n",
    "                      'num_failed_logins', 'num_compromised', 'root_shell', 'su_attempted', 'num_root',\n",
    "                      'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'count',\n",
    "                      'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "                      'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
    "                      'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "                      'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "                      'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
    "\n",
    "numerical_df_full_hl = df_full_hl[numerical_colmanes].copy()\n",
    "print(numerical_df_full_hl.shape)\n",
    "#\n",
    "# # Lets remove the numerical columns with constant value\n",
    "numerical_df_full_hl = numerical_df_full_hl.loc[:, (numerical_df_full_hl != numerical_df_full_hl.iloc[0]).any()]\n",
    "#\n",
    "# # lets scale the values for each column from [0,1]\n",
    "# # N.B. we dont have any negative values]\n",
    "final_df_full_hl = numerical_df_full_hl / numerical_df_full_hl.max()\n",
    "print(final_df_full_hl.shape)\n",
    "\n",
    "df_normalized_hl = pd.concat([final_df_full_hl, threat_type_df_hl], axis=1)\n",
    "print('After normalization shape of data set: ', df_normalized_hl.shape)\n",
    "print(df_normalized_hl[THREAT_TYPE].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def divide_train_test(df, propotion=0.1):\n",
    "    \n",
    "    df_train = []\n",
    "    df_test = []\n",
    "    for key,val in df[THREAT_TYPE].value_counts().iteritems():\n",
    "        df_part = df[df['threat_type'] == key]\n",
    "        df_test.append(df_part[0: int(df_part.shape[0]*propotion)])\n",
    "        df_train.append(df_part[int(df_part.shape[0]*propotion):df_part.shape[0]])\n",
    "        \n",
    "    return df_train,df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data_for_slices(df_train, number_of_slices, isSmote=False, x_name=\"x_train\", y_name=\"y_train\"):\n",
    "    \n",
    "    x_data_dict= dict()\n",
    "    y_data_dict= dict()    \n",
    "    \n",
    "    for i in range(number_of_slices):\n",
    "        xname= x_name+str(i)\n",
    "        yname= y_name+str(i)\n",
    "        df_types = []\n",
    "        \n",
    "        for df in df_train:\n",
    "            df_type = df[int(df.shape[0]*i/number_of_slices):int(df.shape[0]*(i+1)/number_of_slices)]\n",
    "            df_types.append(df_type)\n",
    "        \n",
    "        slice_df = pd.concat(df_types)\n",
    "        y_info = slice_df.pop('threat_type').values\n",
    "        x_info = slice_df.values\n",
    "        y_info = y_info.astype('int')\n",
    "        \n",
    "        if isSmote:\n",
    "            sm = SMOTE(random_state=42)\n",
    "            x_info, y_info = sm.fit_resample(x_info, y_info)\n",
    "        \n",
    "        print('========================================================================================')\n",
    "        print('\\tX part size for slice ' + str(i) + ' is ' + str(x_info.shape))\n",
    "        print('\\tY part size for slice ' + str(i) + ' is ' + str(y_info.shape))\n",
    "        print('Value types of each class in slice : ' + str(i))\n",
    "        print(np.unique(y_info,return_counts=True))\n",
    "        \n",
    "        x_info = torch.tensor(x_info).float()\n",
    "        y_info = torch.tensor(y_info).type(torch.LongTensor)\n",
    "            \n",
    "        x_data_dict.update({xname : x_info})\n",
    "        y_data_dict.update({yname : y_info})\n",
    "        \n",
    "    return x_data_dict, y_data_dict     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================lowlevel==============================================\n",
      "========================================================================================\n",
      "\tX part size for slice 0 is (34670, 33)\n",
      "\tY part size for slice 0 is (34670,)\n",
      "Value types of each class in slice : 0\n",
      "(array([0, 1, 2, 3, 4]), array([6934, 6934, 6934, 6934, 6934], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 1 is (34675, 33)\n",
      "\tY part size for slice 1 is (34675,)\n",
      "Value types of each class in slice : 1\n",
      "(array([0, 1, 2, 3, 4]), array([6935, 6935, 6935, 6935, 6935], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 2 is (34675, 33)\n",
      "\tY part size for slice 2 is (34675,)\n",
      "Value types of each class in slice : 2\n",
      "(array([0, 1, 2, 3, 4]), array([6935, 6935, 6935, 6935, 6935], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 3 is (34675, 33)\n",
      "\tY part size for slice 3 is (34675,)\n",
      "Value types of each class in slice : 3\n",
      "(array([0, 1, 2, 3, 4]), array([6935, 6935, 6935, 6935, 6935], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 4 is (34675, 33)\n",
      "\tY part size for slice 4 is (34675,)\n",
      "Value types of each class in slice : 4\n",
      "(array([0, 1, 2, 3, 4]), array([6935, 6935, 6935, 6935, 6935], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 5 is (34670, 33)\n",
      "\tY part size for slice 5 is (34670,)\n",
      "Value types of each class in slice : 5\n",
      "(array([0, 1, 2, 3, 4]), array([6934, 6934, 6934, 6934, 6934], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 6 is (34675, 33)\n",
      "\tY part size for slice 6 is (34675,)\n",
      "Value types of each class in slice : 6\n",
      "(array([0, 1, 2, 3, 4]), array([6935, 6935, 6935, 6935, 6935], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 7 is (34675, 33)\n",
      "\tY part size for slice 7 is (34675,)\n",
      "Value types of each class in slice : 7\n",
      "(array([0, 1, 2, 3, 4]), array([6935, 6935, 6935, 6935, 6935], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 8 is (34675, 33)\n",
      "\tY part size for slice 8 is (34675,)\n",
      "Value types of each class in slice : 8\n",
      "(array([0, 1, 2, 3, 4]), array([6935, 6935, 6935, 6935, 6935], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 9 is (34675, 33)\n",
      "\tY part size for slice 9 is (34675,)\n",
      "Value types of each class in slice : 9\n",
      "(array([0, 1, 2, 3, 4]), array([6935, 6935, 6935, 6935, 6935], dtype=int64))\n",
      "========================================highlevel==============================================\n",
      "========================================================================================\n",
      "\tX part size for slice 0 is (13868, 33)\n",
      "\tY part size for slice 0 is (13868,)\n",
      "Value types of each class in slice : 0\n",
      "(array([0, 1]), array([6934, 6934], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 1 is (13870, 33)\n",
      "\tY part size for slice 1 is (13870,)\n",
      "Value types of each class in slice : 1\n",
      "(array([0, 1]), array([6935, 6935], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 2 is (13870, 33)\n",
      "\tY part size for slice 2 is (13870,)\n",
      "Value types of each class in slice : 2\n",
      "(array([0, 1]), array([6935, 6935], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 3 is (13870, 33)\n",
      "\tY part size for slice 3 is (13870,)\n",
      "Value types of each class in slice : 3\n",
      "(array([0, 1]), array([6935, 6935], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 4 is (13870, 33)\n",
      "\tY part size for slice 4 is (13870,)\n",
      "Value types of each class in slice : 4\n",
      "(array([0, 1]), array([6935, 6935], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 5 is (13868, 33)\n",
      "\tY part size for slice 5 is (13868,)\n",
      "Value types of each class in slice : 5\n",
      "(array([0, 1]), array([6934, 6934], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 6 is (13870, 33)\n",
      "\tY part size for slice 6 is (13870,)\n",
      "Value types of each class in slice : 6\n",
      "(array([0, 1]), array([6935, 6935], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 7 is (13870, 33)\n",
      "\tY part size for slice 7 is (13870,)\n",
      "Value types of each class in slice : 7\n",
      "(array([0, 1]), array([6935, 6935], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 8 is (13870, 33)\n",
      "\tY part size for slice 8 is (13870,)\n",
      "Value types of each class in slice : 8\n",
      "(array([0, 1]), array([6935, 6935], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 9 is (13870, 33)\n",
      "\tY part size for slice 9 is (13870,)\n",
      "Value types of each class in slice : 9\n",
      "(array([0, 1]), array([6935, 6935], dtype=int64))\n",
      "Test set size is : x => (14849, 33) y => (14849,)\n",
      "Test set size for highlevel is : x => (14851, 33) y => (14851,)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = divide_train_test(df_normalized,propotion=0.1)\n",
    "df_train_hl, df_test_hl = divide_train_test(df_normalized_hl,propotion=0.1)\n",
    "# print('Value counts in train set : ')\n",
    "# df_train[THREAT_TYPE].value_counts()\n",
    "# print('Value counts in test set : ')\n",
    "# print(df_test[THREAT_TYPE].value_counts())\n",
    "\n",
    "print('========================================lowlevel==============================================')\n",
    "x_train_dict, y_train_dict = get_data_for_slices(df_train, number_of_slices, isSmote=True)\n",
    "\n",
    "print('========================================highlevel==============================================')\n",
    "x_train_hl_dict, y_train_hl_dict = get_data_for_slices(df_train_hl, number_of_slices, isSmote=True)\n",
    "\n",
    "df_test_hl = pd.concat(df_test_hl)\n",
    "y_test_hl = df_test_hl.pop(THREAT_TYPE).values\n",
    "x_test_hl = df_test_hl.values\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "y_test = df_test.pop(THREAT_TYPE).values\n",
    "x_test = df_test.values\n",
    "\n",
    "print('Test set size is : x => ' + str(x_test.shape) + ' y => ' + str(y_test.shape))\n",
    "x_test = torch.tensor(x_test).float()\n",
    "y_test = torch.tensor(y_test.astype('int')).type(torch.LongTensor)\n",
    "\n",
    "print('Test set size for highlevel is : x => ' + str(x_test_hl.shape) + ' y => ' + str(y_test_hl.shape))\n",
    "x_test_hl = torch.tensor(x_test_hl).float()\n",
    "y_test_hl = torch.tensor(y_test_hl.astype('int')).type(torch.LongTensor)\n",
    "\n",
    "inputs = x_test.shape[1]\n",
    "inputs_hl = x_test.shape[1]\n",
    "outputs = 5\n",
    "outputs_hl = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "### <span style=\"background-color:#F087F9\"> Classification Model </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net2nn(nn.Module):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super(Net2nn, self).__init__()\n",
    "        self.fc1=nn.Linear(inputs,200)\n",
    "        self.fc2=nn.Linear(200,200)\n",
    "        self.fc3=nn.Linear(200,outputs)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        prediction = output.argmax(dim=1, keepdim=True)\n",
    "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "        \n",
    "\n",
    "    return train_loss / len(train_loader), correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validation(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criterion(output, target).item()\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    return (test_loss, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mat(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # iterate over test data\n",
    "    for inputs, labels in test_loader:\n",
    "        output = model(inputs)  # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output)  # Save Prediction\n",
    "\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels)  # Save Truth\n",
    "\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "#     df_cm = pd.DataFrame(cf_matrix, index=[i for i in Counter(y_test)],\n",
    "#                          columns=[i for i in Counter(y_test)])\n",
    "#     plt.figure(1)\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "#     plt.figure(figsize=(12, 7))\n",
    "\n",
    "#     sn.heatmap(df_cm, annot=True).set(xlabel='Predicted label', ylabel='True label')\n",
    "#     plt.savefig('D:\\\\learning\\\\PyTorch\\\\experiment\\\\cf\\\\cf_fl_'+str(self.number_of_slices)+'.png')\n",
    "    print('confusion matrix for normal scenario for slices : ' + str(number_of_slices))\n",
    "    print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "### <span style=\"background-color:#F087F9\"> Functions for Federated Averaging </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model_optimizer_criterion_dict(number_of_slices, inputs, outputs):\n",
    "    model_dict = dict()\n",
    "    optimizer_dict= dict()\n",
    "    criterion_dict = dict()\n",
    "    \n",
    "    for i in range(number_of_slices):\n",
    "        model_name=\"model\"+str(i)\n",
    "        model_info=Net2nn(inputs, outputs)\n",
    "        model_dict.update({model_name : model_info })\n",
    "        \n",
    "        optimizer_name=\"optimizer\"+str(i)\n",
    "        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        optimizer_dict.update({optimizer_name : optimizer_info })\n",
    "        \n",
    "        criterion_name = \"criterion\"+str(i)\n",
    "        criterion_info = nn.CrossEntropyLoss()\n",
    "        criterion_dict.update({criterion_name : criterion_info})\n",
    "        \n",
    "    return model_dict, optimizer_dict, criterion_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_averaged_weights(model_dict, number_of_slices):\n",
    "   \n",
    "    fc1_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc1.weight.shape)\n",
    "    fc1_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc1.bias.shape)\n",
    "    \n",
    "    fc2_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc2.weight.shape)\n",
    "    fc2_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc2.bias.shape)\n",
    "    \n",
    "    fc3_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc3.weight.shape)\n",
    "    fc3_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc3.bias.shape)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "    \n",
    "        for i in range(number_of_slices):\n",
    "            fc1_mean_weight += model_dict[name_of_models[i]].fc1.weight.data.clone()\n",
    "            fc1_mean_bias += model_dict[name_of_models[i]].fc1.bias.data.clone()\n",
    "        \n",
    "            fc2_mean_weight += model_dict[name_of_models[i]].fc2.weight.data.clone()\n",
    "            fc2_mean_bias += model_dict[name_of_models[i]].fc2.bias.data.clone()\n",
    "        \n",
    "            fc3_mean_weight += model_dict[name_of_models[i]].fc3.weight.data.clone()\n",
    "            fc3_mean_bias += model_dict[name_of_models[i]].fc3.bias.data.clone()\n",
    "\n",
    "        \n",
    "        fc1_mean_weight =fc1_mean_weight/number_of_slices\n",
    "        fc1_mean_bias = fc1_mean_bias/ number_of_slices\n",
    "    \n",
    "        fc2_mean_weight =fc2_mean_weight/number_of_slices\n",
    "        fc2_mean_bias = fc2_mean_bias/ number_of_slices\n",
    "    \n",
    "        fc3_mean_weight =fc3_mean_weight/number_of_slices\n",
    "        fc3_mean_bias = fc3_mean_bias/ number_of_slices\n",
    "    \n",
    "    return fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_slices):\n",
    "    fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias = get_averaged_weights(model_dict, number_of_slices=number_of_slices)\n",
    "    with torch.no_grad():\n",
    "        main_model.fc1.weight.data = fc1_mean_weight.data.clone()\n",
    "        main_model.fc2.weight.data = fc2_mean_weight.data.clone()\n",
    "        main_model.fc3.weight.data = fc3_mean_weight.data.clone()\n",
    "\n",
    "        main_model.fc1.bias.data = fc1_mean_bias.data.clone()\n",
    "        main_model.fc2.bias.data = fc2_mean_bias.data.clone()\n",
    "        main_model.fc3.bias.data = fc3_mean_bias.data.clone() \n",
    "    return main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compare_local_and_merged_model_performance(number_of_slices):\n",
    "    accuracy_table=pd.DataFrame(data=np.zeros((number_of_slices,3)), columns=[\"sample\", \"local_ind_model\", \"merged_main_model\"])\n",
    "    for i in range (number_of_slices):\n",
    "    \n",
    "        test_ds = TensorDataset(x_test, y_test)\n",
    "        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        individual_loss, individual_accuracy = validation(model, test_dl, criterion)\n",
    "        main_loss, main_accuracy =validation(main_model, test_dl, main_criterion )\n",
    "    \n",
    "        accuracy_table.loc[i, \"sample\"]=\"sample \"+str(i)\n",
    "        accuracy_table.loc[i, \"local_ind_model\"] = individual_accuracy\n",
    "        accuracy_table.loc[i, \"merged_main_model\"] = main_accuracy\n",
    "\n",
    "    return accuracy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_slices):\n",
    "    with torch.no_grad():\n",
    "        for i in range(number_of_slices):\n",
    "            print('Updating model :' + name_of_models[i] )\n",
    "            model_dict[name_of_models[i]].fc1.weight.data =main_model.fc1.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.weight.data =main_model.fc2.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.weight.data =main_model.fc3.weight.data.clone() \n",
    "            \n",
    "            model_dict[name_of_models[i]].fc1.bias.data =main_model.fc1.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.bias.data =main_model.fc2.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.bias.data =main_model.fc3.bias.data.clone() \n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def start_train_end_node_process(number_of_slices):\n",
    "    for i in range (number_of_slices): \n",
    "\n",
    "        print('Federated learning for slice '+ str(i+1))\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         valid_ds = TensorDataset(x_valid_dict[name_of_x_valid_sets[i]], y_valid_dict[name_of_y_valid_sets[i]])\n",
    "#         valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n",
    "        \n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        print(\"Subset\" ,i)\n",
    "        for epoch in range(numEpoch):        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "#             valid_loss, valid_accuracy = validation(model, valid_dl, criterion)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "    \n",
    "            print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.5f}\".format(train_accuracy) + \" | test accuracy: {:7.5f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def start_train_end_node_process_without_print(number_of_slices):\n",
    "    for i in range (number_of_slices): \n",
    "\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_ds = TensorDataset(x_test, y_test)\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        for epoch in range(numEpoch):        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def start_train_end_node_process_print_some(number_of_slices, print_amount):\n",
    "    for i in range (number_of_slices): \n",
    "        \n",
    "        print('Federated learning for slice '+ str(i+1))\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], \n",
    "                                 y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_ds = TensorDataset(x_test, y_test)\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        if i<print_amount:\n",
    "            print(\"Subset\" ,i)\n",
    "            \n",
    "        for epoch in range(numEpoch):\n",
    "        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "            \n",
    "            if i<print_amount:        \n",
    "                print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.5f}\".format(train_accuracy) + \" | test accuracy: {:7.5f}\".format(test_accuracy))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x_train, y_train, x_valid, y_valid,x_test, y_test = map(torch.tensor, (x_train, y_train, x_valid, y_valid, x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "### <span style=\"background-color:#F087F9\"> Let's examine what would the performance of the centralized model be if the data were not distributed to nodes at all? </span>   \n",
    "\n",
    "The model used in this example is very simple, different things can be done to improve model performance, such as using more complex models, increasing epoch or hyperparameter tuning. However, the purpose here is to compare the performance of the main model that is formed by combining the parameters of the local models trained on their own data with a centralized model that trained on all training data. In this way, we can gain insight into the capacity of federated learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initial_model = Net2nn()\n",
    "# initial_optimizer = torch.optim.SGD(initial_model.parameters(), lr=0.01, momentum=0.9)\n",
    "# initial_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "centralized_model = Net2nn()\n",
    "centralized_optimizer = torch.optim.SGD(centralized_model.parameters(), lr=0.01, momentum=0.9)\n",
    "centralized_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Centralized Model ------\n",
      "Training with slice 1 data\n",
      "epoch:   1 | train accuracy:  0.8776 | test accuracy:  0.9298\n",
      "epoch:   2 | train accuracy:  0.9327 | test accuracy:  0.9379\n",
      "epoch:   3 | train accuracy:  0.9469 | test accuracy:  0.9433\n",
      "epoch:   4 | train accuracy:  0.9515 | test accuracy:  0.9565\n",
      "epoch:   5 | train accuracy:  0.9554 | test accuracy:  0.9509\n",
      "epoch:   6 | train accuracy:  0.9582 | test accuracy:  0.9608\n",
      "epoch:   7 | train accuracy:  0.9603 | test accuracy:  0.9605\n",
      "epoch:   8 | train accuracy:  0.9628 | test accuracy:  0.9642\n",
      "epoch:   9 | train accuracy:  0.9651 | test accuracy:  0.9681\n",
      "epoch:  10 | train accuracy:  0.9659 | test accuracy:  0.9670\n",
      "confusion matrix for normal scenario for slices : 10\n",
      "[[7442  121    0   86   56]\n",
      " [  58 5273    0    4    3]\n",
      " [   4    0    0    6    1]\n",
      " [  34   27    0  324    3]\n",
      " [  61    8    0   18 1320]]\n",
      "Training with slice 2 data\n",
      "epoch:   1 | train accuracy:  0.8821 | test accuracy:  0.9341\n",
      "epoch:   2 | train accuracy:  0.9376 | test accuracy:  0.9414\n",
      "epoch:   3 | train accuracy:  0.9441 | test accuracy:  0.9412\n",
      "epoch:   4 | train accuracy:  0.9470 | test accuracy:  0.9546\n",
      "epoch:   5 | train accuracy:  0.9516 | test accuracy:  0.9579\n",
      "epoch:   6 | train accuracy:  0.9545 | test accuracy:  0.9604\n",
      "epoch:   7 | train accuracy:  0.9553 | test accuracy:  0.9523\n",
      "epoch:   8 | train accuracy:  0.9574 | test accuracy:  0.9646\n",
      "epoch:   9 | train accuracy:  0.9598 | test accuracy:  0.9667\n",
      "epoch:  10 | train accuracy:  0.9612 | test accuracy:  0.9663\n",
      "confusion matrix for normal scenario for slices : 10\n",
      "[[7563   49    0   60   33]\n",
      " [ 139 5188    0    9    2]\n",
      " [   4    0    0    7    0]\n",
      " [  70    6    0  309    3]\n",
      " [  96    2    0   20 1289]]\n",
      "Training with slice 3 data\n",
      "epoch:   1 | train accuracy:  0.8756 | test accuracy:  0.9248\n",
      "epoch:   2 | train accuracy:  0.9318 | test accuracy:  0.9382\n",
      "epoch:   3 | train accuracy:  0.9382 | test accuracy:  0.9433\n",
      "epoch:   4 | train accuracy:  0.9413 | test accuracy:  0.9461\n",
      "epoch:   5 | train accuracy:  0.9425 | test accuracy:  0.9427\n",
      "epoch:   6 | train accuracy:  0.9446 | test accuracy:  0.9466\n",
      "epoch:   7 | train accuracy:  0.9462 | test accuracy:  0.9428\n",
      "epoch:   8 | train accuracy:  0.9470 | test accuracy:  0.9460\n",
      "epoch:   9 | train accuracy:  0.9486 | test accuracy:  0.9482\n",
      "epoch:  10 | train accuracy:  0.9483 | test accuracy:  0.9497\n",
      "confusion matrix for normal scenario for slices : 10\n",
      "[[7585   51    0   11   58]\n",
      " [ 132 5197    0    0    9]\n",
      " [  10    0    0    0    1]\n",
      " [ 375    2    0    1   10]\n",
      " [  86    2    0    0 1319]]\n",
      "Training with slice 4 data\n",
      "epoch:   1 | train accuracy:  0.8692 | test accuracy:  0.9290\n",
      "epoch:   2 | train accuracy:  0.9335 | test accuracy:  0.9411\n",
      "epoch:   3 | train accuracy:  0.9396 | test accuracy:  0.9450\n",
      "epoch:   4 | train accuracy:  0.9431 | test accuracy:  0.9464\n",
      "epoch:   5 | train accuracy:  0.9451 | test accuracy:  0.9459\n",
      "epoch:   6 | train accuracy:  0.9465 | test accuracy:  0.9453\n",
      "epoch:   7 | train accuracy:  0.9481 | test accuracy:  0.9482\n",
      "epoch:   8 | train accuracy:  0.9502 | test accuracy:  0.9488\n",
      "epoch:   9 | train accuracy:  0.9509 | test accuracy:  0.9499\n",
      "epoch:  10 | train accuracy:  0.9516 | test accuracy:  0.9494\n",
      "confusion matrix for normal scenario for slices : 10\n",
      "[[7585   50    0   15   55]\n",
      " [ 120 5213    0    1    4]\n",
      " [  11    0    0    0    0]\n",
      " [ 371    7    0    1    9]\n",
      " [ 109    0    0    0 1298]]\n",
      "Training with slice 5 data\n",
      "epoch:   1 | train accuracy:  0.8724 | test accuracy:  0.9287\n",
      "epoch:   2 | train accuracy:  0.9347 | test accuracy:  0.9415\n",
      "epoch:   3 | train accuracy:  0.9412 | test accuracy:  0.9426\n",
      "epoch:   4 | train accuracy:  0.9434 | test accuracy:  0.9455\n",
      "epoch:   5 | train accuracy:  0.9442 | test accuracy:  0.9444\n",
      "epoch:   6 | train accuracy:  0.9445 | test accuracy:  0.9477\n",
      "epoch:   7 | train accuracy:  0.9453 | test accuracy:  0.9463\n",
      "epoch:   8 | train accuracy:  0.9469 | test accuracy:  0.9474\n",
      "epoch:   9 | train accuracy:  0.9491 | test accuracy:  0.9461\n",
      "epoch:  10 | train accuracy:  0.9503 | test accuracy:  0.9484\n",
      "confusion matrix for normal scenario for slices : 10\n",
      "[[7632   44    0   17   12]\n",
      " [ 137 5200    0    0    1]\n",
      " [   5    0    0    5    1]\n",
      " [ 381    3    0    1    3]\n",
      " [ 143    5    0    9 1250]]\n",
      "Training with slice 6 data\n",
      "epoch:   1 | train accuracy:  0.8740 | test accuracy:  0.9315\n",
      "epoch:   2 | train accuracy:  0.9339 | test accuracy:  0.9419\n",
      "epoch:   3 | train accuracy:  0.9410 | test accuracy:  0.9454\n",
      "epoch:   4 | train accuracy:  0.9438 | test accuracy:  0.9461\n",
      "epoch:   5 | train accuracy:  0.9451 | test accuracy:  0.9465\n",
      "epoch:   6 | train accuracy:  0.9466 | test accuracy:  0.9476\n",
      "epoch:   7 | train accuracy:  0.9475 | test accuracy:  0.9481\n",
      "epoch:   8 | train accuracy:  0.9483 | test accuracy:  0.9490\n",
      "epoch:   9 | train accuracy:  0.9513 | test accuracy:  0.9487\n",
      "epoch:  10 | train accuracy:  0.9526 | test accuracy:  0.9502\n",
      "confusion matrix for normal scenario for slices : 10\n",
      "[[7562   52    0    3   88]\n",
      " [ 124 5211    0    0    3]\n",
      " [  11    0    0    0    0]\n",
      " [ 202    7    0    1  178]\n",
      " [  69    3    0    0 1335]]\n",
      "Training with slice 7 data\n",
      "epoch:   1 | train accuracy:  0.8783 | test accuracy:  0.9262\n",
      "epoch:   2 | train accuracy:  0.9352 | test accuracy:  0.9420\n",
      "epoch:   3 | train accuracy:  0.9412 | test accuracy:  0.9456\n",
      "epoch:   4 | train accuracy:  0.9435 | test accuracy:  0.9441\n",
      "epoch:   5 | train accuracy:  0.9440 | test accuracy:  0.9446\n",
      "epoch:   6 | train accuracy:  0.9449 | test accuracy:  0.9483\n",
      "epoch:   7 | train accuracy:  0.9453 | test accuracy:  0.9472\n",
      "epoch:   8 | train accuracy:  0.9486 | test accuracy:  0.9483\n",
      "epoch:   9 | train accuracy:  0.9498 | test accuracy:  0.9507\n",
      "epoch:  10 | train accuracy:  0.9501 | test accuracy:  0.9446\n",
      "confusion matrix for normal scenario for slices : 10\n",
      "[[7375  208    0   33   89]\n",
      " [  44 5291    0    3    0]\n",
      " [   4    0    0    4    3]\n",
      " [ 168   25    0   34  161]\n",
      " [  59   12    0   10 1326]]\n",
      "Training with slice 8 data\n",
      "epoch:   1 | train accuracy:  0.8614 | test accuracy:  0.9316\n",
      "epoch:   2 | train accuracy:  0.9351 | test accuracy:  0.9417\n",
      "epoch:   3 | train accuracy:  0.9418 | test accuracy:  0.9445\n",
      "epoch:   4 | train accuracy:  0.9425 | test accuracy:  0.9423\n",
      "epoch:   5 | train accuracy:  0.9433 | test accuracy:  0.9432\n",
      "epoch:   6 | train accuracy:  0.9453 | test accuracy:  0.9450\n",
      "epoch:   7 | train accuracy:  0.9454 | test accuracy:  0.9485\n",
      "epoch:   8 | train accuracy:  0.9468 | test accuracy:  0.9494\n",
      "epoch:   9 | train accuracy:  0.9482 | test accuracy:  0.9494\n",
      "epoch:  10 | train accuracy:  0.9494 | test accuracy:  0.9467\n",
      "confusion matrix for normal scenario for slices : 10\n",
      "[[7629    7    0    1   68]\n",
      " [ 221 5098    0    0   19]\n",
      " [  11    0    0    0    0]\n",
      " [ 313    2    0    0   73]\n",
      " [  76    1    0    0 1330]]\n",
      "Training with slice 9 data\n",
      "epoch:   1 | train accuracy:  0.8333 | test accuracy:  0.9005\n",
      "epoch:   2 | train accuracy:  0.9036 | test accuracy:  0.9210\n",
      "epoch:   3 | train accuracy:  0.9144 | test accuracy:  0.9250\n",
      "epoch:   4 | train accuracy:  0.9212 | test accuracy:  0.9295\n",
      "epoch:   5 | train accuracy:  0.9252 | test accuracy:  0.9325\n",
      "epoch:   6 | train accuracy:  0.9284 | test accuracy:  0.9286\n",
      "epoch:   7 | train accuracy:  0.9312 | test accuracy:  0.9335\n",
      "epoch:   8 | train accuracy:  0.9307 | test accuracy:  0.9361\n",
      "epoch:   9 | train accuracy:  0.9341 | test accuracy:  0.9358\n",
      "epoch:  10 | train accuracy:  0.9369 | test accuracy:  0.9358\n",
      "confusion matrix for normal scenario for slices : 10\n",
      "[[7607   62    0    0   36]\n",
      " [ 199 5129    0    0   10]\n",
      " [  11    0    0    0    0]\n",
      " [ 376    9    0    0    3]\n",
      " [ 231   17    0    0 1159]]\n",
      "Training with slice 10 data\n",
      "epoch:   1 | train accuracy:  0.8187 | test accuracy:  0.8737\n",
      "epoch:   2 | train accuracy:  0.9043 | test accuracy:  0.8746\n",
      "epoch:   3 | train accuracy:  0.9133 | test accuracy:  0.8905\n",
      "epoch:   4 | train accuracy:  0.9200 | test accuracy:  0.8851\n",
      "epoch:   5 | train accuracy:  0.9252 | test accuracy:  0.8896\n",
      "epoch:   6 | train accuracy:  0.9253 | test accuracy:  0.8964\n",
      "epoch:   7 | train accuracy:  0.9256 | test accuracy:  0.8904\n",
      "epoch:   8 | train accuracy:  0.9325 | test accuracy:  0.9011\n",
      "epoch:   9 | train accuracy:  0.9288 | test accuracy:  0.9004\n",
      "epoch:  10 | train accuracy:  0.9357 | test accuracy:  0.8945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix for normal scenario for slices : 10\n",
      "[[7015  274    0  101  315]\n",
      " [ 231 5067    0   13   27]\n",
      " [   9    0    0    1    1]\n",
      " [ 269   20    0   93    6]\n",
      " [ 206   81    0   13 1107]]\n",
      "------ Training finished ------\n",
      "Mean train accuracy: 0.9345378139016661\n",
      "Mean test accuracy: 0.9390800727321702\n"
     ]
    }
   ],
   "source": [
    "print(\"------ Centralized Model ------\")\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "test_ds = TensorDataset(x_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n",
    "\n",
    "for i in range(number_of_slices):\n",
    "    centralized_model = Net2nn()\n",
    "    centralized_optimizer = torch.optim.SGD(centralized_model.parameters(), lr=0.01, momentum=0.9)\n",
    "    centralized_criterion = nn.CrossEntropyLoss()\n",
    "#     centralized_model = copy.deepcopy(initial_model)\n",
    "#     centralized_optimizer = copy.deepcopy(initial_optimizer)\n",
    "#     centralized_criterion = copy.deepcopy(initial_criterion)\n",
    "    print('Training with slice ' + str(i+1) + ' data' )\n",
    "    x_name = 'x_train' + str(i)\n",
    "    y_name = 'y_train' + str(i)\n",
    "    train_ds = TensorDataset(x_train_dict[x_name], y_train_dict[y_name])\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(numEpoch):\n",
    "        central_train_loss, central_train_accuracy = train(centralized_model, train_dl, centralized_criterion, centralized_optimizer)\n",
    "        central_test_loss, central_test_accuracy = validation(centralized_model, test_dl, centralized_criterion)\n",
    "        \n",
    "        train_acc.append(central_train_accuracy)\n",
    "        train_loss.append(central_train_loss)\n",
    "        test_acc.append(central_test_accuracy)\n",
    "        test_loss.append(central_test_loss)\n",
    "        \n",
    "        print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.4f}\".format(central_train_accuracy) + \" | test accuracy: {:7.4f}\".format(central_test_accuracy))\n",
    "    confusion_mat(centralized_model, test_dl)\n",
    "    \n",
    "print(\"------ Training finished ------\")\n",
    "print('Mean train accuracy: ' + str(sum(train_acc)/len(train_acc)))\n",
    "print('Mean test accuracy: ' + str(sum(test_acc)/len(test_acc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------\n",
    "-----------------\n",
    "**Data is distributed to nodes**\n",
    "\n",
    "<!-- ### <span style=\"background-color:#F087F9\"> DatanÄ±n nodelara daÄÄ±tÄ±lmasÄ± </span>    -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13367, 33]) torch.Size([13367])\n",
      "torch.Size([14849, 33]) torch.Size([14849])\n"
     ]
    }
   ],
   "source": [
    "print(x_train_dict[\"x_train1\"].shape, y_train_dict[\"y_train1\"].shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main model is created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_model = Net2nn()\n",
    "main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "main_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models,optimizers and loss functions in nodes are defined**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict(number_of_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keys of dicts are being made iterable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_train0', 'x_train1', 'x_train2', 'x_train3', 'x_train4', 'x_train5', 'x_train6', 'x_train7', 'x_train8', 'x_train9']\n",
      "['y_train0', 'y_train1', 'y_train2', 'y_train3', 'y_train4', 'y_train5', 'y_train6', 'y_train7', 'y_train8', 'y_train9']\n",
      "\n",
      " ------------\n",
      "['model0', 'model1', 'model2', 'model3', 'model4', 'model5', 'model6', 'model7', 'model8', 'model9']\n",
      "['optimizer0', 'optimizer1', 'optimizer2', 'optimizer3', 'optimizer4', 'optimizer5', 'optimizer6', 'optimizer7', 'optimizer8', 'optimizer9']\n",
      "['criterion0', 'criterion1', 'criterion2', 'criterion3', 'criterion4', 'criterion5', 'criterion6', 'criterion7', 'criterion8', 'criterion9']\n"
     ]
    }
   ],
   "source": [
    "name_of_x_train_sets=list(x_train_dict.keys())\n",
    "name_of_y_train_sets=list(y_train_dict.keys())\n",
    "\n",
    "name_of_models=list(model_dict.keys())\n",
    "name_of_optimizers=list(optimizer_dict.keys())\n",
    "name_of_criterions=list(criterion_dict.keys())\n",
    "\n",
    "print(name_of_x_train_sets)\n",
    "print(name_of_y_train_sets)\n",
    "print(\"\\n ------------\")\n",
    "print(name_of_models)\n",
    "print(name_of_optimizers)\n",
    "print(name_of_criterions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0362,  0.0192,  0.0541,  0.0572, -0.0380]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[-0.0323, -0.0439,  0.0121, -0.0199,  0.0115]],\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(main_model.fc2.weight[0:1,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters of main model are sent to nodes**  \n",
    "Since the parameters of the main model and parameters of all local models in the nodes are randomly initialized, all these parameters will be different from each other. For this reason, the main model sends its parameters to the nodes before the training of local models in the nodes begins. You can check the weights below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Updating model :model4\n",
      "Updating model :model5\n",
      "Updating model :model6\n",
      "Updating model :model7\n",
      "Updating model :model8\n",
      "Updating model :model9\n"
     ]
    }
   ],
   "source": [
    "model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0362,  0.0192,  0.0541,  0.0572, -0.0380]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[ 0.0362,  0.0192,  0.0541,  0.0572, -0.0380]],\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(main_model.fc2.weight[0:1,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models in the nodes are trained**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated learning for slice 1\n",
      "Subset 0\n",
      "epoch:   1 | train accuracy: 0.86943 | test accuracy: 0.93057\n",
      "epoch:   2 | train accuracy: 0.93453 | test accuracy: 0.95427\n",
      "epoch:   3 | train accuracy: 0.94799 | test accuracy: 0.95293\n",
      "epoch:   4 | train accuracy: 0.95136 | test accuracy: 0.95185\n",
      "epoch:   5 | train accuracy: 0.95495 | test accuracy: 0.95757\n",
      "epoch:   6 | train accuracy: 0.95735 | test accuracy: 0.96067\n",
      "epoch:   7 | train accuracy: 0.95959 | test accuracy: 0.96195\n",
      "epoch:   8 | train accuracy: 0.96244 | test accuracy: 0.95690\n",
      "epoch:   9 | train accuracy: 0.96356 | test accuracy: 0.95980\n",
      "epoch:  10 | train accuracy: 0.96528 | test accuracy: 0.97023\n",
      "Federated learning for slice 2\n",
      "Subset 1\n",
      "epoch:   1 | train accuracy: 0.87454 | test accuracy: 0.93171\n",
      "epoch:   2 | train accuracy: 0.93693 | test accuracy: 0.93926\n",
      "epoch:   3 | train accuracy: 0.94299 | test accuracy: 0.94451\n",
      "epoch:   4 | train accuracy: 0.94808 | test accuracy: 0.95131\n",
      "epoch:   5 | train accuracy: 0.95190 | test accuracy: 0.95710\n",
      "epoch:   6 | train accuracy: 0.95324 | test accuracy: 0.95461\n",
      "epoch:   7 | train accuracy: 0.95459 | test accuracy: 0.96310\n",
      "epoch:   8 | train accuracy: 0.95683 | test accuracy: 0.96498\n",
      "epoch:   9 | train accuracy: 0.96013 | test accuracy: 0.96552\n",
      "epoch:  10 | train accuracy: 0.95998 | test accuracy: 0.96680\n",
      "Federated learning for slice 3\n",
      "Subset 2\n",
      "epoch:   1 | train accuracy: 0.86960 | test accuracy: 0.92767\n",
      "epoch:   2 | train accuracy: 0.93357 | test accuracy: 0.94053\n",
      "epoch:   3 | train accuracy: 0.93948 | test accuracy: 0.94336\n",
      "epoch:   4 | train accuracy: 0.94187 | test accuracy: 0.94323\n",
      "epoch:   5 | train accuracy: 0.94307 | test accuracy: 0.94599\n",
      "epoch:   6 | train accuracy: 0.94277 | test accuracy: 0.94276\n",
      "epoch:   7 | train accuracy: 0.94546 | test accuracy: 0.94208\n",
      "epoch:   8 | train accuracy: 0.94629 | test accuracy: 0.94922\n",
      "epoch:   9 | train accuracy: 0.94898 | test accuracy: 0.94969\n",
      "epoch:  10 | train accuracy: 0.94980 | test accuracy: 0.94963\n",
      "Federated learning for slice 4\n",
      "Federated learning for slice 5\n",
      "Federated learning for slice 6\n",
      "Federated learning for slice 7\n",
      "Federated learning for slice 8\n",
      "Federated learning for slice 9\n",
      "Federated learning for slice 10\n"
     ]
    }
   ],
   "source": [
    "# start_train_end_node_process()\n",
    "start_train_end_node_process_print_some(number_of_slices, print_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0362,  0.0192,  0.0541,  0.0572, -0.0380], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.0510,  0.0538,  0.1233,  0.0120, -0.0382], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "## As you can see, wieghts of local models are updated after training process\n",
    "print(main_model.fc2.weight[0,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare the performance of federated main model, individual local models and centralized model  \n",
    "\n",
    "**Federated main model vs individual local models before 1st iteration (on distributed test set)**  \n",
    "Since main model is randomly initialized and no action taken on it yet, its performance is very poor. Please before_acc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix for normal scenario for slices : 10\n",
      "[[5968    0    3    0 1734]\n",
      " [1179    0    0    0 4159]\n",
      " [   1    0    0    0   10]\n",
      " [  18    0    0    0  370]\n",
      " [ 570    0    1    0  836]]\n",
      "confusion matrix for normal scenario for slices : 10\n",
      "[[7582   73    0    0   50]\n",
      " [ 137 5198    0    0    3]\n",
      " [  10    0    0    0    1]\n",
      " [ 371    8    0    0    9]\n",
      " [ 112    3    0    0 1292]]\n"
     ]
    }
   ],
   "source": [
    "before_acc_table=compare_local_and_merged_model_performance(number_of_slices=number_of_slices)\n",
    "before_test_loss, before_test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "confusion_mat(main_model, test_dl)\n",
    "\n",
    "main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_slices) \n",
    "\n",
    "after_acc_table=compare_local_and_merged_model_performance(number_of_slices=number_of_slices)\n",
    "after_test_loss, after_test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "confusion_mat(main_model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated main model vs individual local models before FedAvg first iteration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>local_ind_model</th>\n",
       "      <th>merged_main_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample 0</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.4582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample 1</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>0.4582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample 2</td>\n",
       "      <td>0.9496</td>\n",
       "      <td>0.4582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample 3</td>\n",
       "      <td>0.9504</td>\n",
       "      <td>0.4582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample 4</td>\n",
       "      <td>0.9494</td>\n",
       "      <td>0.4582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample  local_ind_model  merged_main_model\n",
       "0  sample 0           0.9702             0.4582\n",
       "1  sample 1           0.9668             0.4582\n",
       "2  sample 2           0.9496             0.4582\n",
       "3  sample 3           0.9504             0.4582\n",
       "4  sample 4           0.9494             0.4582"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Federated main model vs individual local models before FedAvg first iteration\")\n",
    "before_acc_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated main model vs individual local models after FedAvg first iteration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>local_ind_model</th>\n",
       "      <th>merged_main_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample 0</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.9477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample 1</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>0.9477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample 2</td>\n",
       "      <td>0.9496</td>\n",
       "      <td>0.9477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample 3</td>\n",
       "      <td>0.9504</td>\n",
       "      <td>0.9477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample 4</td>\n",
       "      <td>0.9494</td>\n",
       "      <td>0.9477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample  local_ind_model  merged_main_model\n",
       "0  sample 0           0.9702             0.9477\n",
       "1  sample 1           0.9668             0.9477\n",
       "2  sample 2           0.9496             0.9477\n",
       "3  sample 3           0.9504             0.9477\n",
       "4  sample 4           0.9494             0.9477"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Federated main model vs individual local models after FedAvg first iteration\")\n",
    "after_acc_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Federated main model vs centralized model before 1st iteration (on all test data)**  \n",
    "Please be aware that the centralized model gets approximately %98 accuracy on all test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before 1st iteration main model accuracy on all test data:  0.4582\n",
      "After 1st iteration main model accuracy on all test data:  0.9477\n",
      "Centralized model accuracy on all test data:  0.8945\n"
     ]
    }
   ],
   "source": [
    "print(\"Before 1st iteration main model accuracy on all test data: {:7.4f}\".format(before_test_accuracy))\n",
    "print(\"After 1st iteration main model accuracy on all test data: {:7.4f}\".format(after_test_accuracy))\n",
    "print(\"Centralized model accuracy on all test data: {:7.4f}\".format(central_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a single iteration, we can send the weights of the main model back to the nodes and repeat the above steps.\n",
    "Now let's check how the performance of the main model improves when we repeat the iteration 10 more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Updating model :model4\n",
      "Updating model :model5\n",
      "Updating model :model6\n",
      "Updating model :model7\n",
      "Updating model :model8\n",
      "Updating model :model9\n",
      "Iteration 2 : main_model accuracy on all test data:  0.9648\n",
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Updating model :model4\n",
      "Updating model :model5\n",
      "Updating model :model6\n",
      "Updating model :model7\n",
      "Updating model :model8\n",
      "Updating model :model9\n",
      "Iteration 3 : main_model accuracy on all test data:  0.9677\n",
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Updating model :model4\n",
      "Updating model :model5\n",
      "Updating model :model6\n",
      "Updating model :model7\n",
      "Updating model :model8\n",
      "Updating model :model9\n",
      "Iteration 4 : main_model accuracy on all test data:  0.9682\n",
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Updating model :model4\n",
      "Updating model :model5\n",
      "Updating model :model6\n",
      "Updating model :model7\n",
      "Updating model :model8\n",
      "Updating model :model9\n",
      "Iteration 5 : main_model accuracy on all test data:  0.9704\n",
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Updating model :model4\n",
      "Updating model :model5\n",
      "Updating model :model6\n",
      "Updating model :model7\n",
      "Updating model :model8\n",
      "Updating model :model9\n",
      "Iteration 6 : main_model accuracy on all test data:  0.9703\n",
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Updating model :model4\n",
      "Updating model :model5\n",
      "Updating model :model6\n",
      "Updating model :model7\n",
      "Updating model :model8\n",
      "Updating model :model9\n",
      "Iteration 7 : main_model accuracy on all test data:  0.9727\n",
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Updating model :model4\n",
      "Updating model :model5\n",
      "Updating model :model6\n",
      "Updating model :model7\n",
      "Updating model :model8\n",
      "Updating model :model9\n",
      "Iteration 8 : main_model accuracy on all test data:  0.9735\n",
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Updating model :model4\n",
      "Updating model :model5\n",
      "Updating model :model6\n",
      "Updating model :model7\n",
      "Updating model :model8\n",
      "Updating model :model9\n",
      "Iteration 9 : main_model accuracy on all test data:  0.9745\n",
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Updating model :model4\n",
      "Updating model :model5\n",
      "Updating model :model6\n",
      "Updating model :model7\n",
      "Updating model :model8\n",
      "Updating model :model9\n",
      "Iteration 10 : main_model accuracy on all test data:  0.9749\n",
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Updating model :model4\n",
      "Updating model :model5\n",
      "Updating model :model6\n",
      "Updating model :model7\n",
      "Updating model :model8\n",
      "Updating model :model9\n",
      "Iteration 11 : main_model accuracy on all test data:  0.9749\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_slices)\n",
    "    start_train_end_node_process_without_print(number_of_slices)\n",
    "    main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_slices) \n",
    "    test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "    print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the centralized model was calculated as approximately 98%. The accuracy of the main model obtained by FedAvg method started from 85% and improved to 94%. In this case, we can say that although the main model obtained by FedAvg method was trained without seeing the data, its performance cannot be underestimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
