{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#6B49F5> A Simple Implementation of FedAvg with PyTorch on IIDÂ Data </font> \n",
    "Please see https://towardsdatascience.com/federated-learning-a-simple-implementation-of-fedavg-federated-averaging-with-pytorch-90187c9c9577 for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pd.options.display.float_format = \"{:,.4f}\".format\n",
    "sm = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "THREAT_TYPE = 'threat_type'\n",
    "THREAT_HL = 'threat_hl'\n",
    "\n",
    "learning_rate = 0.01\n",
    "numEpoch = 20\n",
    "batch_size = 32\n",
    "momentum = 0.9\n",
    "print_amount=3\n",
    "number_of_slices = 2\n",
    "isSmote = False\n",
    "runtime = 21\n",
    "\n",
    "file_name = \"federated_\" + str(isSmote) + \"_\" + str(number_of_slices)  + \"_\" + str(runtime) + \".txt\"\n",
    "file = open(file_name, \"w\")\n",
    "\n",
    "data_path = \"D:\\\\learning\\\\PyTorch\\\\NSL_KDD-master\\\\\"\n",
    "\n",
    "colnames = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n",
    "            'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
    "            'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files',\n",
    "            'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
    "            'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "            'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "            'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "            'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "            'dst_host_srv_rerror_rate', 'threat_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack types in full set: \n",
      " 0    77053\n",
      "1    53387\n",
      "4    14077\n",
      "3     3880\n",
      "2      119\n",
      "Name: threat_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(data_path + \"KDDTrain+.csv\", header = None)\n",
    "df_train = df_train.iloc[:, :-1]\n",
    "\n",
    "df_test = pd.read_csv(data_path + \"KDDTest+.csv\", header = None)\n",
    "df_test = df_test.iloc[:, :-1]\n",
    "\n",
    "df_train.columns = colnames\n",
    "df_test.columns = colnames\n",
    "\n",
    "df_train.loc[(df_train['threat_type'] == 'back'), 'threat_type'] = 1\n",
    "df_train.loc[(df_train['threat_type'] == 'buffer_overflow'), 'threat_type'] = 2\n",
    "df_train.loc[(df_train['threat_type'] == 'ftp_write'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'guess_passwd'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'imap'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'ipsweep'), 'threat_type'] = 4\n",
    "df_train.loc[(df_train['threat_type'] == 'land'), 'threat_type'] = 1\n",
    "df_train.loc[(df_train['threat_type'] == 'loadmodule'), 'threat_type'] = 2\n",
    "df_train.loc[(df_train['threat_type'] == 'multihop'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'neptune'), 'threat_type'] = 1\n",
    "df_train.loc[(df_train['threat_type'] == 'nmap'), 'threat_type'] = 4\n",
    "df_train.loc[(df_train['threat_type'] == 'perl'), 'threat_type'] = 2\n",
    "df_train.loc[(df_train['threat_type'] == 'phf'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'pod'), 'threat_type'] = 1\n",
    "df_train.loc[(df_train['threat_type'] == 'portsweep'), 'threat_type'] = 4\n",
    "df_train.loc[(df_train['threat_type'] == 'rootkit'), 'threat_type'] = 2\n",
    "df_train.loc[(df_train['threat_type'] == 'satan'), 'threat_type'] = 4\n",
    "df_train.loc[(df_train['threat_type'] == 'smurf'), 'threat_type'] = 1\n",
    "df_train.loc[(df_train['threat_type'] == 'spy'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'teardrop'), 'threat_type'] = 1\n",
    "df_train.loc[(df_train['threat_type'] == 'warezclient'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'warezmaster'), 'threat_type'] = 3\n",
    "df_train.loc[(df_train['threat_type'] == 'normal'), 'threat_type'] = 0\n",
    "df_train.loc[(df_train['threat_type'] == 'unknown'), 'threat_type'] = 6\n",
    "\n",
    "df_test.loc[(df_test['threat_type'] == 'back'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'buffer_overflow'), 'threat_type'] = 2\n",
    "df_test.loc[(df_test['threat_type'] == 'ftp_write'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'guess_passwd'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'imap'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'ipsweep'), 'threat_type'] = 4\n",
    "df_test.loc[(df_test['threat_type'] == 'land'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'loadmodule'), 'threat_type'] = 2\n",
    "df_test.loc[(df_test['threat_type'] == 'multihop'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'neptune'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'nmap'), 'threat_type'] = 4\n",
    "df_test.loc[(df_test['threat_type'] == 'perl'), 'threat_type'] = 2\n",
    "df_test.loc[(df_test['threat_type'] == 'phf'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'pod'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'portsweep'), 'threat_type'] = 4\n",
    "df_test.loc[(df_test['threat_type'] == 'rootkit'), 'threat_type'] = 2\n",
    "df_test.loc[(df_test['threat_type'] == 'satan'), 'threat_type'] = 4\n",
    "df_test.loc[(df_test['threat_type'] == 'smurf'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'spy'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'teardrop'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'warezclient'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'warezmaster'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'normal'), 'threat_type'] = 0\n",
    "df_test.loc[(df_test['threat_type'] == 'unknown'), 'threat_type'] = 6\n",
    "df_test.loc[(df_test['threat_type'] == 'mscan'), 'threat_type'] = 4\n",
    "df_test.loc[(df_test['threat_type'] == 'apache2'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'snmpgetattack'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'processtable'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'httptunnel'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'ps'), 'threat_type'] = 2\n",
    "df_test.loc[(df_test['threat_type'] == 'snmpguess'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'mailbomb'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'named'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'sendmail'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'xterm'), 'threat_type'] = 2\n",
    "df_test.loc[(df_test['threat_type'] == 'xlock'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'xsnoop'), 'threat_type'] = 3\n",
    "df_test.loc[(df_test['threat_type'] == 'sqlattack'), 'threat_type'] = 2\n",
    "df_test.loc[(df_test['threat_type'] == 'udpstorm'), 'threat_type'] = 1\n",
    "df_test.loc[(df_test['threat_type'] == 'saint'), 'threat_type'] = 4\n",
    "df_test.loc[(df_test['threat_type'] == 'worm'), 'threat_type'] = 1\n",
    "\n",
    "df_full = pd.concat([df_train, df_test])\n",
    "\n",
    "print('Attack types in full set: \\n', df_full[THREAT_TYPE].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization shape of data set :  (148516, 42)\n",
      "(148516, 34)\n",
      "(148516, 33)\n",
      "After normalization shape of data set:  (148516, 34)\n",
      "0    77053\n",
      "1    53387\n",
      "4    14077\n",
      "3     3880\n",
      "2      119\n",
      "Name: threat_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Before normalization shape of data set : ', df_full.shape)\n",
    "threat_type_df = df_full['threat_type'].copy()\n",
    "# Considering numerical columns\n",
    "# 34 numerical columns are considered for training\n",
    "numerical_colmanes = ['duration', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'urgent', 'hot',\n",
    "                      'num_failed_logins', 'num_compromised', 'root_shell', 'su_attempted', 'num_root',\n",
    "                      'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'count',\n",
    "                      'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "                      'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
    "                      'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "                      'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "                      'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
    "\n",
    "numerical_df_full = df_full[numerical_colmanes].copy()\n",
    "print(numerical_df_full.shape)\n",
    "#\n",
    "# # Lets remove the numerical columns with constant value\n",
    "numerical_df_full = numerical_df_full.loc[:, (numerical_df_full != numerical_df_full.iloc[0]).any()]\n",
    "#\n",
    "# # lets scale the values for each column from [0,1]\n",
    "# # N.B. we dont have any negative values]\n",
    "final_df_full = numerical_df_full / numerical_df_full.max()\n",
    "print(final_df_full.shape)\n",
    "\n",
    "df_normalized = pd.concat([final_df_full, threat_type_df], axis=1)\n",
    "print('After normalization shape of data set: ', df_normalized.shape)\n",
    "print(df_normalized[THREAT_TYPE].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def divide_train_test(df, propotion=0.1):\n",
    "    \n",
    "    df_train = []\n",
    "    df_test = []\n",
    "    for key,val in df[THREAT_TYPE].value_counts().iteritems():\n",
    "        df_part = df[df['threat_type'] == key]\n",
    "        df_test.append(df_part[0: int(df_part.shape[0]*propotion)])\n",
    "        df_train.append(df_part[int(df_part.shape[0]*propotion):df_part.shape[0]])\n",
    "        \n",
    "    return df_train,df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data_for_slices(df_train, number_of_slices, isSmote=False, x_name=\"x_train\", y_name=\"y_train\"):\n",
    "    \n",
    "    x_data_dict= dict()\n",
    "    y_data_dict= dict()    \n",
    "    \n",
    "    for i in range(number_of_slices):\n",
    "        xname= x_name+str(i)\n",
    "        yname= y_name+str(i)\n",
    "        df_types = []\n",
    "        \n",
    "        for df in df_train:\n",
    "            df_type = df[int(df.shape[0]*i/number_of_slices):int(df.shape[0]*(i+1)/number_of_slices)]\n",
    "            df_types.append(df_type)\n",
    "        \n",
    "        slice_df = pd.concat(df_types)\n",
    "        y_info = slice_df.pop('threat_type').values\n",
    "        x_info = slice_df.values\n",
    "        y_info = y_info.astype('int')\n",
    "        \n",
    "        if isSmote:\n",
    "            sm = SMOTE(random_state=42)\n",
    "            x_info, y_info = sm.fit_resample(x_info, y_info)\n",
    "        \n",
    "        print('========================================================================================')\n",
    "        print('\\tX part size for slice ' + str(i) + ' is ' + str(x_info.shape))\n",
    "        print('\\tY part size for slice ' + str(i) + ' is ' + str(y_info.shape))\n",
    "        print('Value types of each class in slice : ' + str(i))\n",
    "        print(np.unique(y_info,return_counts=True))\n",
    "        \n",
    "        x_info = torch.tensor(x_info).float()\n",
    "        y_info = torch.tensor(y_info).type(torch.LongTensor)\n",
    "            \n",
    "        x_data_dict.update({xname : x_info})\n",
    "        y_data_dict.update({yname : y_info})\n",
    "        \n",
    "    return x_data_dict, y_data_dict     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       duration  src_bytes  dst_bytes  wrong_fragment  urgent    hot  \\\n",
      "14478    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "14481    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "14482    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "14483    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "14487    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "...         ...        ...        ...             ...     ...    ...   \n",
      "22532    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22534    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22538    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22539    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22541    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "\n",
      "       num_failed_logins  num_compromised  root_shell  su_attempted  ...  \\\n",
      "14478             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "14481             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "14482             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "14483             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "14487             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "...                  ...              ...         ...           ...  ...   \n",
      "22532             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22534             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22538             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22539             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22541             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "\n",
      "       dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "14478              1.0000                  1.0000                  0.0000   \n",
      "14481              1.0000                  1.0000                  0.0000   \n",
      "14482              1.0000                  1.0000                  0.0000   \n",
      "14483              1.0000                  1.0000                  0.0000   \n",
      "14487              0.1725                  0.4200                  0.0400   \n",
      "...                   ...                     ...                     ...   \n",
      "22532              1.0000                  1.0000                  0.0000   \n",
      "22534              1.0000                  1.0000                  0.0000   \n",
      "22538              0.5529                  0.7200                  0.0600   \n",
      "22539              1.0000                  1.0000                  0.0000   \n",
      "22541              0.9882                  0.9900                  0.0100   \n",
      "\n",
      "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "14478                       0.0100                       0.0000   \n",
      "14481                       0.0000                       0.0000   \n",
      "14482                       0.3300                       0.0300   \n",
      "14483                       0.0200                       0.0600   \n",
      "14487                       0.4200                       0.0500   \n",
      "...                            ...                          ...   \n",
      "22532                       0.0100                       0.0400   \n",
      "22534                       0.2000                       0.0400   \n",
      "22538                       0.0100                       0.0100   \n",
      "22539                       0.0100                       0.0100   \n",
      "22541                       0.0000                       0.0000   \n",
      "\n",
      "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "14478                0.0000                    0.0000                0.0000   \n",
      "14481                0.0000                    0.0000                0.0000   \n",
      "14482                0.0000                    0.0000                0.0000   \n",
      "14483                0.0000                    0.0000                0.0000   \n",
      "14487                0.0000                    0.0000                0.0000   \n",
      "...                     ...                       ...                   ...   \n",
      "22532                0.0000                    0.0000                0.0000   \n",
      "22534                0.0000                    0.0000                0.0000   \n",
      "22538                0.0100                    0.0000                0.0000   \n",
      "22539                0.0100                    0.0000                0.0000   \n",
      "22541                0.0000                    0.0000                0.0000   \n",
      "\n",
      "       dst_host_srv_rerror_rate  threat_type  \n",
      "14478                    0.0000            0  \n",
      "14481                    0.0000            0  \n",
      "14482                    0.0000            0  \n",
      "14483                    0.0000            0  \n",
      "14487                    0.0000            0  \n",
      "...                         ...          ...  \n",
      "22532                    0.0000            0  \n",
      "22534                    0.0000            0  \n",
      "22538                    0.0000            0  \n",
      "22539                    0.0000            0  \n",
      "22541                    0.0000            0  \n",
      "\n",
      "[69348 rows x 34 columns],        duration  src_bytes  dst_bytes  wrong_fragment  urgent    hot  \\\n",
      "14449    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "14450    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "14456    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "14459    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "14465    0.0000     0.0000     0.0000          0.3333  0.0000 0.0000   \n",
      "...         ...        ...        ...             ...     ...    ...   \n",
      "22533    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22535    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22536    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22537    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22540    0.0000     0.0000     0.0000          0.0000  0.0000 0.0198   \n",
      "\n",
      "       num_failed_logins  num_compromised  root_shell  su_attempted  ...  \\\n",
      "14449             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "14450             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "14456             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "14459             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "14465             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "...                  ...              ...         ...           ...  ...   \n",
      "22533             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22535             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22536             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22537             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22540             0.0000           0.0001      0.0000        0.0000  ...   \n",
      "\n",
      "       dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "14449              0.0431                  0.0400                  0.0700   \n",
      "14450              0.2078                  0.2100                  0.0400   \n",
      "14456              0.0157                  0.0200                  0.0800   \n",
      "14459              0.0745                  0.0700                  0.0600   \n",
      "14465              0.0745                  0.0700                  0.0200   \n",
      "...                   ...                     ...                     ...   \n",
      "22533              0.0392                  0.0400                  0.0700   \n",
      "22535              0.0706                  0.0700                  0.0500   \n",
      "22536              0.7294                  0.7300                  0.1300   \n",
      "22537              1.0000                  1.0000                  0.0000   \n",
      "22540              1.0000                  1.0000                  0.0000   \n",
      "\n",
      "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "14449                       0.0000                       0.0000   \n",
      "14450                       0.0100                       0.0000   \n",
      "14456                       0.0000                       0.0000   \n",
      "14459                       0.0000                       0.0000   \n",
      "14465                       0.0700                       0.0000   \n",
      "...                            ...                          ...   \n",
      "22533                       0.0000                       0.0000   \n",
      "22535                       0.0000                       0.0000   \n",
      "22536                       0.0000                       0.0000   \n",
      "22537                       1.0000                       0.0000   \n",
      "22540                       0.0000                       0.0000   \n",
      "\n",
      "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "14449                1.0000                    1.0000                0.0000   \n",
      "14450                1.0000                    1.0000                0.0000   \n",
      "14456                1.0000                    1.0000                0.0000   \n",
      "14459                1.0000                    1.0000                0.0000   \n",
      "14465                0.0000                    0.0000                0.0000   \n",
      "...                     ...                       ...                   ...   \n",
      "22533                0.0000                    0.0000                1.0000   \n",
      "22535                0.0000                    0.0000                1.0000   \n",
      "22536                0.0000                    0.0000                0.2600   \n",
      "22537                0.0000                    0.0000                0.0000   \n",
      "22540                0.0000                    0.0000                0.0700   \n",
      "\n",
      "       dst_host_srv_rerror_rate  threat_type  \n",
      "14449                    0.0000            1  \n",
      "14450                    0.0000            1  \n",
      "14456                    0.0000            1  \n",
      "14459                    0.0000            1  \n",
      "14465                    0.0000            1  \n",
      "...                         ...          ...  \n",
      "22533                    1.0000            1  \n",
      "22535                    1.0000            1  \n",
      "22536                    0.0000            1  \n",
      "22537                    0.0000            1  \n",
      "22540                    0.0700            1  \n",
      "\n",
      "[48049 rows x 34 columns],        duration  src_bytes  dst_bytes  wrong_fragment  urgent    hot  \\\n",
      "15700    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "15702    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "15706    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "15709    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "15731    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "...         ...        ...        ...             ...     ...    ...   \n",
      "22503    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22508    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22519    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22520    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22542    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "\n",
      "       num_failed_logins  num_compromised  root_shell  su_attempted  ...  \\\n",
      "15700             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "15702             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "15706             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "15709             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "15731             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "...                  ...              ...         ...           ...  ...   \n",
      "22503             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22508             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22519             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22520             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22542             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "\n",
      "       dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "15700              0.4824                  1.0000                  0.0000   \n",
      "15702              0.3529                  1.0000                  0.0000   \n",
      "15706              0.1373                  1.0000                  0.0000   \n",
      "15709              0.0039                  0.0000                  0.5900   \n",
      "15731              0.6314                  1.0000                  0.0000   \n",
      "...                   ...                     ...                     ...   \n",
      "22503              0.0039                  0.0000                  1.0000   \n",
      "22508              0.3294                  0.3300                  0.0300   \n",
      "22519              0.0039                  0.0000                  0.1400   \n",
      "22520              0.0039                  0.0000                  0.5400   \n",
      "22542              0.0824                  0.0800                  0.0300   \n",
      "\n",
      "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "15700                       1.0000                       0.5000   \n",
      "15702                       1.0000                       0.5000   \n",
      "15706                       1.0000                       0.5100   \n",
      "15709                       0.0000                       0.0000   \n",
      "15731                       1.0000                       0.2500   \n",
      "...                            ...                          ...   \n",
      "22503                       0.0000                       0.0000   \n",
      "22508                       0.0100                       0.0200   \n",
      "22519                       0.0000                       0.0000   \n",
      "22520                       0.9700                       0.0000   \n",
      "22542                       0.0000                       0.0000   \n",
      "\n",
      "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "15700                0.0000                    0.0000                0.0000   \n",
      "15702                0.0000                    0.0000                0.0000   \n",
      "15706                0.0000                    0.0000                0.0000   \n",
      "15709                0.0000                    0.0000                0.5500   \n",
      "15731                0.0000                    0.0000                0.0000   \n",
      "...                     ...                       ...                   ...   \n",
      "22503                0.0600                    0.0000                0.9000   \n",
      "22508                0.0000                    0.0000                0.8800   \n",
      "22519                0.0400                    0.0000                0.8500   \n",
      "22520                0.0000                    0.0000                0.0300   \n",
      "22542                0.0000                    0.0000                0.4400   \n",
      "\n",
      "       dst_host_srv_rerror_rate  threat_type  \n",
      "15700                    0.0000            4  \n",
      "15702                    0.0000            4  \n",
      "15706                    0.0000            4  \n",
      "15709                    1.0000            4  \n",
      "15731                    0.0000            4  \n",
      "...                         ...          ...  \n",
      "22503                    1.0000            4  \n",
      "22508                    0.6900            4  \n",
      "22519                    1.0000            4  \n",
      "22520                    0.0000            4  \n",
      "22542                    1.0000            4  \n",
      "\n",
      "[12670 rows x 34 columns],        duration  src_bytes  dst_bytes  wrong_fragment  urgent    hot  \\\n",
      "45323    0.0000     0.0000     0.0000          0.0000  0.0000 0.2772   \n",
      "45325    0.0000     0.0000     0.0000          0.0000  0.0000 0.0099   \n",
      "45528    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "45562    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "45674    0.0000     0.0000     0.0000          0.0000  0.0000 0.2772   \n",
      "...         ...        ...        ...             ...     ...    ...   \n",
      "22485    0.0049     0.0000     0.0000          0.0000  0.0000 0.0198   \n",
      "22487    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22489    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22500    0.0001     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22512    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "\n",
      "       num_failed_logins  num_compromised  root_shell  su_attempted  ...  \\\n",
      "45323             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "45325             0.2000           0.0000      0.0000        0.0000  ...   \n",
      "45528             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "45562             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "45674             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "...                  ...              ...         ...           ...  ...   \n",
      "22485             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22487             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22489             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22500             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22512             0.2000           0.0000      0.0000        0.0000  ...   \n",
      "\n",
      "       dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "45323              0.0235                  0.0300                  0.0300   \n",
      "45325              0.0784                  1.0000                  0.0000   \n",
      "45528              0.0353                  1.0000                  0.0000   \n",
      "45562              0.0941                  1.0000                  0.0000   \n",
      "45674              0.0118                  0.0200                  0.0300   \n",
      "...                   ...                     ...                     ...   \n",
      "22485              0.0706                  0.0700                  0.9300   \n",
      "22487              0.0745                  0.0700                  0.0200   \n",
      "22489              0.0392                  0.0400                  0.0300   \n",
      "22500              0.7020                  0.7000                  0.0200   \n",
      "22512              0.1412                  0.1400                  0.0200   \n",
      "\n",
      "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "45323                       0.0100                       0.0000   \n",
      "45325                       0.0500                       0.0000   \n",
      "45528                       1.0000                       0.3300   \n",
      "45562                       1.0000                       0.1200   \n",
      "45674                       0.0100                       0.0000   \n",
      "...                            ...                          ...   \n",
      "22485                       0.0000                       0.0000   \n",
      "22487                       0.0000                       0.0000   \n",
      "22489                       0.0000                       0.0000   \n",
      "22500                       0.0000                       0.0000   \n",
      "22512                       0.0000                       0.0000   \n",
      "\n",
      "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "45323                0.0100                    0.0000                0.0100   \n",
      "45325                0.0500                    0.0500                0.9500   \n",
      "45528                0.0000                    0.0000                0.0000   \n",
      "45562                0.0000                    0.0000                0.0000   \n",
      "45674                0.0100                    0.0000                0.0100   \n",
      "...                     ...                       ...                   ...   \n",
      "22485                0.0000                    0.0000                0.9300   \n",
      "22487                0.0000                    0.0000                0.0800   \n",
      "22489                0.0100                    0.0000                0.0000   \n",
      "22500                0.0000                    0.0000                0.1100   \n",
      "22512                0.0000                    0.0000                0.0000   \n",
      "\n",
      "       dst_host_srv_rerror_rate  threat_type  \n",
      "45323                    0.0000            3  \n",
      "45325                    0.9500            3  \n",
      "45528                    0.0000            3  \n",
      "45562                    0.0000            3  \n",
      "45674                    0.0000            3  \n",
      "...                         ...          ...  \n",
      "22485                    0.0000            3  \n",
      "22487                    0.9500            3  \n",
      "22489                    0.0000            3  \n",
      "22500                    0.0000            3  \n",
      "22512                    0.0300            3  \n",
      "\n",
      "[3492 rows x 34 columns],        duration  src_bytes  dst_bytes  wrong_fragment  urgent    hot  \\\n",
      "25326    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "25636    0.0008     0.0000     0.0000          0.0000  0.0000 0.0396   \n",
      "27430    0.0000     0.0000     0.0000          0.0000  0.0000 0.0099   \n",
      "30533    0.0030     0.0000     0.0000          0.0000  0.0000 0.0297   \n",
      "36730    0.0026     0.0000     0.0000          0.0000  0.0000 0.0099   \n",
      "...         ...        ...        ...             ...     ...    ...   \n",
      "21191    0.0119     0.0000     0.0000          0.0000  0.0000 0.0099   \n",
      "21245    0.0001     0.0000     0.0000          0.0000  0.0000 0.0099   \n",
      "21935    0.0000     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22084    0.0007     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "22523    0.0003     0.0000     0.0000          0.0000  0.0000 0.0000   \n",
      "\n",
      "       num_failed_logins  num_compromised  root_shell  su_attempted  ...  \\\n",
      "25326             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "25636             0.0000           0.0003      1.0000        0.0000  ...   \n",
      "27430             0.0000           0.0000      1.0000        0.0000  ...   \n",
      "30533             0.0000           0.0005      1.0000        0.0000  ...   \n",
      "36730             0.0000           0.0004      0.0000        0.0000  ...   \n",
      "...                  ...              ...         ...           ...  ...   \n",
      "21191             0.2000           0.0011      0.0000        0.0000  ...   \n",
      "21245             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "21935             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "22084             0.0000           0.0000      1.0000        0.0000  ...   \n",
      "22523             0.0000           0.0000      0.0000        0.0000  ...   \n",
      "\n",
      "       dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "25326              0.0039                  1.0000                  0.0000   \n",
      "25636              0.0078                  1.0000                  0.0000   \n",
      "27430              0.0157                  1.0000                  0.0000   \n",
      "30533              0.0314                  1.0000                  0.0000   \n",
      "36730              0.0039                  1.0000                  0.0000   \n",
      "...                   ...                     ...                     ...   \n",
      "21191              0.0549                  0.0500                  0.7100   \n",
      "21245              0.0784                  0.7400                  0.1100   \n",
      "21935              0.0039                  1.0000                  0.0000   \n",
      "22084              0.0510                  0.0500                  0.0200   \n",
      "22523              0.0157                  0.0200                  0.0300   \n",
      "\n",
      "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "25326                       1.0000                       0.0000   \n",
      "25636                       1.0000                       1.0000   \n",
      "27430                       1.0000                       0.0000   \n",
      "30533                       0.1200                       0.0000   \n",
      "36730                       1.0000                       0.0000   \n",
      "...                            ...                          ...   \n",
      "21191                       0.0000                       0.0000   \n",
      "21245                       0.0400                       0.0000   \n",
      "21935                       1.0000                       0.0000   \n",
      "22084                       0.0000                       0.0000   \n",
      "22523                       0.0000                       0.0000   \n",
      "\n",
      "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "25326                0.0000                    0.0000                0.0000   \n",
      "25636                0.0000                    0.0000                0.0000   \n",
      "27430                0.0000                    0.0000                0.0000   \n",
      "30533                0.0000                    0.0000                0.1200   \n",
      "36730                0.0000                    0.0000                0.0000   \n",
      "...                     ...                       ...                   ...   \n",
      "21191                0.0000                    0.0000                0.6800   \n",
      "21245                0.0000                    0.0000                0.0000   \n",
      "21935                0.0000                    0.0000                0.0000   \n",
      "22084                0.0000                    0.0000                0.0300   \n",
      "22523                0.0000                    0.0000                0.0000   \n",
      "\n",
      "       dst_host_srv_rerror_rate  threat_type  \n",
      "25326                    0.0000            2  \n",
      "25636                    0.0000            2  \n",
      "27430                    0.0000            2  \n",
      "30533                    0.1200            2  \n",
      "36730                    0.0000            2  \n",
      "...                         ...          ...  \n",
      "21191                    0.0700            2  \n",
      "21245                    0.0000            2  \n",
      "21935                    0.0000            2  \n",
      "22084                    0.6200            2  \n",
      "22523                    0.0000            2  \n",
      "\n",
      "[108 rows x 34 columns]]\n",
      "========================================================================================\n",
      "\tX part size for slice 0 is (66833, 33)\n",
      "\tY part size for slice 0 is (66833,)\n",
      "Value types of each class in slice : 0\n",
      "(array([0, 1, 2, 3, 4]), array([34674, 24024,    54,  1746,  6335], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 1 is (66834, 33)\n",
      "\tY part size for slice 1 is (66834,)\n",
      "Value types of each class in slice : 1\n",
      "(array([0, 1, 2, 3, 4]), array([34674, 24025,    54,  1746,  6335], dtype=int64))\n",
      "Test set size is : x => (14849, 33) y => (14849,)\n",
      "33 5\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = divide_train_test(df_normalized,propotion=0.1)\n",
    "print(df_train)\n",
    "# print('Value counts in train set : ')\n",
    "# df_train[THREAT_TYPE].value_counts()\n",
    "# print('Value counts in test set : ')\n",
    "# print(df_test[THREAT_TYPE].value_counts())\n",
    "\n",
    "x_train_dict, y_train_dict = get_data_for_slices(df_train, number_of_slices, isSmote)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "y_test = df_test.pop(THREAT_TYPE).values\n",
    "x_test = df_test.values\n",
    "\n",
    "print('Test set size is : x => ' + str(x_test.shape) + ' y => ' + str(y_test.shape))\n",
    "x_test = torch.tensor(x_test).float()\n",
    "y_test = torch.tensor(y_test.astype('int')).type(torch.LongTensor)\n",
    "\n",
    "inputs = x_test.shape[1]\n",
    "outputs = 5\n",
    "print(inputs,outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "### <span style=\"background-color:#F087F9\"> Classification Model </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net2nn(nn.Module):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super(Net2nn, self).__init__()\n",
    "        self.fc1=nn.Linear(inputs,200)\n",
    "        self.fc2=nn.Linear(200,200)\n",
    "        self.fc3=nn.Linear(200,outputs)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        prediction = output.argmax(dim=1, keepdim=True)\n",
    "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "        \n",
    "\n",
    "    return train_loss / len(train_loader), correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validation(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criterion(output, target).item()\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    return (test_loss, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mat(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # iterate over test data\n",
    "    for inputs, labels in test_loader:\n",
    "        output = model(inputs)  # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output)  # Save Prediction\n",
    "\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels)  # Save Truth\n",
    "\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    precisionv = precision_score(y_true,y_pred,average='macro')\n",
    "    recallv = recall_score(y_true,y_pred,average='macro')\n",
    "    print('precision value: '+str(precisionv))\n",
    "    print('recall value: '+ str(recallv))\n",
    "#     df_cm = pd.DataFrame(cf_matrix, index=[i for i in Counter(y_test)],\n",
    "#                          columns=[i for i in Counter(y_test)])\n",
    "#     plt.figure(1)\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "#     plt.figure(figsize=(12, 7))\n",
    "\n",
    "#     sn.heatmap(df_cm, annot=True).set(xlabel='Predicted label', ylabel='True label')\n",
    "#     plt.savefig('D:\\\\learning\\\\PyTorch\\\\experiment\\\\cf\\\\cf_fl_'+str(self.number_of_slices)+'.png')\n",
    "    print('confusion matrix for normal scenario for slices : ' + str(number_of_slices))\n",
    "    print(cf_matrix)\n",
    "    file.write('\\ncf matrix for slice :' + str(number_of_slices))\n",
    "    file.write('\\n'+str(cf_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "### <span style=\"background-color:#F087F9\"> Functions for Federated Averaging </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model_optimizer_criterion_dict(number_of_slices):\n",
    "    model_dict = dict()\n",
    "    optimizer_dict= dict()\n",
    "    criterion_dict = dict()\n",
    "    \n",
    "    for i in range(number_of_slices):\n",
    "        model_name=\"model\"+str(i)\n",
    "        model_info=Net2nn(inputs, outputs)\n",
    "        model_dict.update({model_name : model_info })\n",
    "        \n",
    "        optimizer_name=\"optimizer\"+str(i)\n",
    "        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        optimizer_dict.update({optimizer_name : optimizer_info })\n",
    "        \n",
    "        criterion_name = \"criterion\"+str(i)\n",
    "        criterion_info = nn.CrossEntropyLoss()\n",
    "        criterion_dict.update({criterion_name : criterion_info})\n",
    "        \n",
    "    return model_dict, optimizer_dict, criterion_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_averaged_weights(model_dict, number_of_slices):\n",
    "   \n",
    "    fc1_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc1.weight.shape)\n",
    "    fc1_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc1.bias.shape)\n",
    "    \n",
    "    fc2_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc2.weight.shape)\n",
    "    fc2_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc2.bias.shape)\n",
    "    \n",
    "    fc3_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc3.weight.shape)\n",
    "    fc3_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc3.bias.shape)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "    \n",
    "        for i in range(number_of_slices):\n",
    "            fc1_mean_weight += model_dict[name_of_models[i]].fc1.weight.data.clone()\n",
    "            fc1_mean_bias += model_dict[name_of_models[i]].fc1.bias.data.clone()\n",
    "        \n",
    "            fc2_mean_weight += model_dict[name_of_models[i]].fc2.weight.data.clone()\n",
    "            fc2_mean_bias += model_dict[name_of_models[i]].fc2.bias.data.clone()\n",
    "        \n",
    "            fc3_mean_weight += model_dict[name_of_models[i]].fc3.weight.data.clone()\n",
    "            fc3_mean_bias += model_dict[name_of_models[i]].fc3.bias.data.clone()\n",
    "\n",
    "        \n",
    "        fc1_mean_weight =fc1_mean_weight/number_of_slices\n",
    "        fc1_mean_bias = fc1_mean_bias/ number_of_slices\n",
    "    \n",
    "        fc2_mean_weight =fc2_mean_weight/number_of_slices\n",
    "        fc2_mean_bias = fc2_mean_bias/ number_of_slices\n",
    "    \n",
    "        fc3_mean_weight =fc3_mean_weight/number_of_slices\n",
    "        fc3_mean_bias = fc3_mean_bias/ number_of_slices\n",
    "    \n",
    "    return fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_slices):\n",
    "    fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias = get_averaged_weights(model_dict, number_of_slices=number_of_slices)\n",
    "    with torch.no_grad():\n",
    "        main_model.fc1.weight.data = fc1_mean_weight.data.clone()\n",
    "        main_model.fc2.weight.data = fc2_mean_weight.data.clone()\n",
    "        main_model.fc3.weight.data = fc3_mean_weight.data.clone()\n",
    "\n",
    "        main_model.fc1.bias.data = fc1_mean_bias.data.clone()\n",
    "        main_model.fc2.bias.data = fc2_mean_bias.data.clone()\n",
    "        main_model.fc3.bias.data = fc3_mean_bias.data.clone() \n",
    "    return main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compare_local_and_merged_model_performance(number_of_slices):\n",
    "    accuracy_table=pd.DataFrame(data=np.zeros((number_of_slices,3)), columns=[\"sample\", \"local_ind_model\", \"merged_main_model\"])\n",
    "    for i in range (number_of_slices):\n",
    "    \n",
    "        test_ds = TensorDataset(x_test, y_test)\n",
    "        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        individual_loss, individual_accuracy = validation(model, test_dl, criterion)\n",
    "        main_loss, main_accuracy =validation(main_model, test_dl, main_criterion )\n",
    "    \n",
    "        accuracy_table.loc[i, \"sample\"]=\"sample \"+str(i)\n",
    "        accuracy_table.loc[i, \"local_ind_model\"] = individual_accuracy\n",
    "        accuracy_table.loc[i, \"merged_main_model\"] = main_accuracy\n",
    "\n",
    "    return accuracy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_slices):\n",
    "    with torch.no_grad():\n",
    "        for i in range(number_of_slices):\n",
    "            print('Updating model :' + name_of_models[i] )\n",
    "            model_dict[name_of_models[i]].fc1.weight.data =main_model.fc1.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.weight.data =main_model.fc2.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.weight.data =main_model.fc3.weight.data.clone() \n",
    "            \n",
    "            model_dict[name_of_models[i]].fc1.bias.data =main_model.fc1.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.bias.data =main_model.fc2.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.bias.data =main_model.fc3.bias.data.clone() \n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def start_train_end_node_process(number_of_slices):\n",
    "    for i in range (number_of_slices): \n",
    "\n",
    "        print('Federated learning for slice '+ str(i+1))\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         valid_ds = TensorDataset(x_valid_dict[name_of_x_valid_sets[i]], y_valid_dict[name_of_y_valid_sets[i]])\n",
    "#         valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n",
    "        \n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        print(\"Subset\" ,i)\n",
    "        for epoch in range(numEpoch):        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "#             valid_loss, valid_accuracy = validation(model, valid_dl, criterion)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "    \n",
    "            print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.5f}\".format(train_accuracy) + \" | test accuracy: {:7.5f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def start_train_end_node_process_without_print(number_of_slices):\n",
    "    for i in range (number_of_slices): \n",
    "\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_ds = TensorDataset(x_test, y_test)\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        for epoch in range(numEpoch):        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def start_train_end_node_process_print_some(number_of_slices, print_amount):\n",
    "    for i in range (number_of_slices): \n",
    "        \n",
    "        print('Federated learning for slice '+ str(i+1))\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], \n",
    "                                 y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_ds = TensorDataset(x_test, y_test)\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        if i<print_amount:\n",
    "            print(\"Subset\" ,i)\n",
    "            \n",
    "        for epoch in range(numEpoch):\n",
    "        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "            \n",
    "            if i<print_amount:        \n",
    "                print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.5f}\".format(train_accuracy) + \" | test accuracy: {:7.5f}\".format(test_accuracy))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x_train, y_train, x_valid, y_valid,x_test, y_test = map(torch.tensor, (x_train, y_train, x_valid, y_valid, x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "### <span style=\"background-color:#F087F9\"> Let's examine what would the performance of the centralized model be if the data were not distributed to nodes at all? </span>   \n",
    "\n",
    "The model used in this example is very simple, different things can be done to improve model performance, such as using more complex models, increasing epoch or hyperparameter tuning. However, the purpose here is to compare the performance of the main model that is formed by combining the parameters of the local models trained on their own data with a centralized model that trained on all training data. In this way, we can gain insight into the capacity of federated learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initial_model = Net2nn()\n",
    "# initial_optimizer = torch.optim.SGD(initial_model.parameters(), lr=0.01, momentum=0.9)\n",
    "# initial_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "centralized_model = Net2nn(inputs, outputs)\n",
    "centralized_optimizer = torch.optim.SGD(centralized_model.parameters(), lr=0.01, momentum=0.9)\n",
    "centralized_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Centralized Model ------\n",
      "Training with slice 1 data\n",
      " | train accuracy:  0.9738 | test accuracy:  0.9789\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"numpy.float64\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#         print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.4f}\".format(central_train_accuracy) + \" | test accuracy: {:7.4f}\".format(central_test_accuracy))\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m | train accuracy: \u001b[39m\u001b[38;5;132;01m{:7.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(central_train_accuracy) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m | test accuracy: \u001b[39m\u001b[38;5;132;01m{:7.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(central_test_accuracy))\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mconfusion_mat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcentralized_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------ Training finished ------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean train accuracy: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28msum\u001b[39m(train_acc)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_acc)))\n",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36mconfusion_mat\u001b[1;34m(model, test_loader)\u001b[0m\n\u001b[0;32m     16\u001b[0m     precisionv \u001b[38;5;241m=\u001b[39m precision_score(y_true,y_pred,average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m     recallv \u001b[38;5;241m=\u001b[39m recall_score(y_true,y_pred,average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprecision value: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mprecisionv\u001b[49m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall value: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m recallv)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#     df_cm = pd.DataFrame(cf_matrix, index=[i for i in Counter(y_test)],\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#                          columns=[i for i in Counter(y_test)])\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#     plt.figure(1)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#     sn.heatmap(df_cm, annot=True).set(xlabel='Predicted label', ylabel='True label')\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#     plt.savefig('D:\\\\learning\\\\PyTorch\\\\experiment\\\\cf\\\\cf_fl_'+str(self.number_of_slices)+'.png')\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"numpy.float64\") to str"
     ]
    }
   ],
   "source": [
    "print(\"------ Centralized Model ------\")\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "test_ds = TensorDataset(x_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n",
    "\n",
    "for i in range(number_of_slices):\n",
    "    centralized_model = Net2nn(inputs,outputs)\n",
    "    centralized_optimizer = torch.optim.SGD(centralized_model.parameters(), lr=0.01, momentum=0.9)\n",
    "    centralized_criterion = nn.CrossEntropyLoss()\n",
    "#     centralized_model = copy.deepcopy(initial_model)\n",
    "#     centralized_optimizer = copy.deepcopy(initial_optimizer)\n",
    "#     centralized_criterion = copy.deepcopy(initial_criterion)\n",
    "    print('Training with slice ' + str(i+1) + ' data' )\n",
    "    x_name = 'x_train' + str(i)\n",
    "    y_name = 'y_train' + str(i)\n",
    "    train_ds = TensorDataset(x_train_dict[x_name], y_train_dict[y_name])\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(numEpoch):\n",
    "        central_train_loss, central_train_accuracy = train(centralized_model, train_dl, centralized_criterion, centralized_optimizer)\n",
    "        central_test_loss, central_test_accuracy = validation(centralized_model, test_dl, centralized_criterion)\n",
    "        \n",
    "        train_acc.append(central_train_accuracy)\n",
    "        train_loss.append(central_train_loss)\n",
    "        test_acc.append(central_test_accuracy)\n",
    "        test_loss.append(central_test_loss)\n",
    "        \n",
    "#         print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.4f}\".format(central_train_accuracy) + \" | test accuracy: {:7.4f}\".format(central_test_accuracy))\n",
    "    print(\" | train accuracy: {:7.4f}\".format(central_train_accuracy) + \" | test accuracy: {:7.4f}\".format(central_test_accuracy))\n",
    "    confusion_mat(centralized_model, test_dl)\n",
    "    \n",
    "print(\"------ Training finished ------\")\n",
    "print('Mean train accuracy: ' + str(sum(train_acc)/len(train_acc)))\n",
    "print('Mean test accuracy: ' + str(sum(test_acc)/len(test_acc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.write('\\nCentralized Mean train accuracy: ' + str(sum(train_acc)/len(train_acc)))\n",
    "file.write('\\nCentralized Mean test accuracy: ' + str(sum(test_acc)/len(test_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------\n",
    "-----------------\n",
    "**Data is distributed to nodes**\n",
    "\n",
    "<!-- ### <span style=\"background-color:#F087F9\"> DatanÄ±n nodelara daÄÄ±tÄ±lmasÄ± </span>    -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33417, 33]) torch.Size([33417])\n",
      "torch.Size([14849, 33]) torch.Size([14849])\n"
     ]
    }
   ],
   "source": [
    "print(x_train_dict[\"x_train1\"].shape, y_train_dict[\"y_train1\"].shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main model is created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_model = Net2nn(inputs,outputs)\n",
    "main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "main_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models,optimizers and loss functions in nodes are defined**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict(number_of_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keys of dicts are being made iterable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_train0', 'x_train1', 'x_train2', 'x_train3']\n",
      "['y_train0', 'y_train1', 'y_train2', 'y_train3']\n",
      "\n",
      " ------------\n",
      "['model0', 'model1', 'model2', 'model3']\n",
      "['optimizer0', 'optimizer1', 'optimizer2', 'optimizer3']\n",
      "['criterion0', 'criterion1', 'criterion2', 'criterion3']\n"
     ]
    }
   ],
   "source": [
    "name_of_x_train_sets=list(x_train_dict.keys())\n",
    "name_of_y_train_sets=list(y_train_dict.keys())\n",
    "\n",
    "name_of_models=list(model_dict.keys())\n",
    "name_of_optimizers=list(optimizer_dict.keys())\n",
    "name_of_criterions=list(criterion_dict.keys())\n",
    "\n",
    "print(name_of_x_train_sets)\n",
    "print(name_of_y_train_sets)\n",
    "print(\"\\n ------------\")\n",
    "print(name_of_models)\n",
    "print(name_of_optimizers)\n",
    "print(name_of_criterions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0459,  0.0528,  0.0031,  0.0325, -0.0223]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[ 0.0564,  0.0337,  0.0494, -0.0080, -0.0444]],\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(main_model.fc2.weight[0:1,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters of main model are sent to nodes**  \n",
    "Since the parameters of the main model and parameters of all local models in the nodes are randomly initialized, all these parameters will be different from each other. For this reason, the main model sends its parameters to the nodes before the training of local models in the nodes begins. You can check the weights below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n"
     ]
    }
   ],
   "source": [
    "model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0459,  0.0528,  0.0031,  0.0325, -0.0223]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[ 0.0459,  0.0528,  0.0031,  0.0325, -0.0223]],\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(main_model.fc2.weight[0:1,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models in the nodes are trained**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated learning for slice 1\n",
      "Subset 0\n",
      "epoch:   1 | train accuracy: 0.90938 | test accuracy: 0.94222\n",
      "epoch:   2 | train accuracy: 0.94712 | test accuracy: 0.95373\n",
      "epoch:   3 | train accuracy: 0.95302 | test accuracy: 0.96114\n",
      "epoch:   4 | train accuracy: 0.95715 | test accuracy: 0.96128\n",
      "epoch:   5 | train accuracy: 0.96047 | test accuracy: 0.97030\n",
      "epoch:   6 | train accuracy: 0.96247 | test accuracy: 0.96943\n",
      "epoch:   7 | train accuracy: 0.96490 | test accuracy: 0.97010\n",
      "epoch:   8 | train accuracy: 0.96553 | test accuracy: 0.97151\n",
      "epoch:   9 | train accuracy: 0.96693 | test accuracy: 0.97205\n",
      "epoch:  10 | train accuracy: 0.96741 | test accuracy: 0.96963\n",
      "epoch:  11 | train accuracy: 0.96843 | test accuracy: 0.97380\n",
      "epoch:  12 | train accuracy: 0.96882 | test accuracy: 0.97266\n",
      "epoch:  13 | train accuracy: 0.96867 | test accuracy: 0.97757\n",
      "epoch:  14 | train accuracy: 0.96986 | test accuracy: 0.97367\n",
      "epoch:  15 | train accuracy: 0.97097 | test accuracy: 0.97690\n",
      "epoch:  16 | train accuracy: 0.97085 | test accuracy: 0.97400\n",
      "epoch:  17 | train accuracy: 0.97163 | test accuracy: 0.97582\n",
      "epoch:  18 | train accuracy: 0.97205 | test accuracy: 0.97495\n",
      "epoch:  19 | train accuracy: 0.97226 | test accuracy: 0.97380\n",
      "epoch:  20 | train accuracy: 0.97352 | test accuracy: 0.97629\n",
      "Federated learning for slice 2\n",
      "Subset 1\n",
      "epoch:   1 | train accuracy: 0.90888 | test accuracy: 0.94303\n",
      "epoch:   2 | train accuracy: 0.94284 | test accuracy: 0.94814\n",
      "epoch:   3 | train accuracy: 0.94637 | test accuracy: 0.94767\n",
      "epoch:   4 | train accuracy: 0.94982 | test accuracy: 0.94889\n",
      "epoch:   5 | train accuracy: 0.95299 | test accuracy: 0.95407\n",
      "epoch:   6 | train accuracy: 0.95580 | test accuracy: 0.95501\n",
      "epoch:   7 | train accuracy: 0.95858 | test accuracy: 0.95939\n",
      "epoch:   8 | train accuracy: 0.96017 | test accuracy: 0.96155\n",
      "epoch:   9 | train accuracy: 0.96143 | test accuracy: 0.95912\n",
      "epoch:  10 | train accuracy: 0.96167 | test accuracy: 0.95899\n",
      "epoch:  11 | train accuracy: 0.96280 | test accuracy: 0.95468\n",
      "epoch:  12 | train accuracy: 0.96367 | test accuracy: 0.95818\n",
      "epoch:  13 | train accuracy: 0.96460 | test accuracy: 0.96276\n",
      "epoch:  14 | train accuracy: 0.96514 | test accuracy: 0.95730\n",
      "epoch:  15 | train accuracy: 0.96601 | test accuracy: 0.96182\n",
      "epoch:  16 | train accuracy: 0.96627 | test accuracy: 0.96491\n",
      "epoch:  17 | train accuracy: 0.96774 | test accuracy: 0.96640\n",
      "epoch:  18 | train accuracy: 0.96711 | test accuracy: 0.95858\n",
      "epoch:  19 | train accuracy: 0.96789 | test accuracy: 0.96397\n",
      "epoch:  20 | train accuracy: 0.96837 | test accuracy: 0.95771\n",
      "Federated learning for slice 3\n",
      "Subset 2\n",
      "epoch:   1 | train accuracy: 0.91115 | test accuracy: 0.93697\n",
      "epoch:   2 | train accuracy: 0.94278 | test accuracy: 0.94316\n",
      "epoch:   3 | train accuracy: 0.94580 | test accuracy: 0.94821\n",
      "epoch:   4 | train accuracy: 0.94907 | test accuracy: 0.95865\n",
      "epoch:   5 | train accuracy: 0.95323 | test accuracy: 0.95138\n",
      "epoch:   6 | train accuracy: 0.95529 | test accuracy: 0.94969\n",
      "epoch:   7 | train accuracy: 0.95807 | test accuracy: 0.95293\n",
      "epoch:   8 | train accuracy: 0.95825 | test accuracy: 0.95825\n",
      "epoch:   9 | train accuracy: 0.96143 | test accuracy: 0.95535\n",
      "epoch:  10 | train accuracy: 0.96116 | test accuracy: 0.95279\n",
      "epoch:  11 | train accuracy: 0.96202 | test accuracy: 0.95407\n",
      "epoch:  12 | train accuracy: 0.96409 | test accuracy: 0.95144\n",
      "epoch:  13 | train accuracy: 0.96298 | test accuracy: 0.96404\n",
      "epoch:  14 | train accuracy: 0.96391 | test accuracy: 0.95690\n",
      "epoch:  15 | train accuracy: 0.96562 | test accuracy: 0.96283\n",
      "epoch:  16 | train accuracy: 0.96532 | test accuracy: 0.96707\n",
      "epoch:  17 | train accuracy: 0.96538 | test accuracy: 0.96424\n",
      "epoch:  18 | train accuracy: 0.96615 | test accuracy: 0.96229\n",
      "epoch:  19 | train accuracy: 0.96702 | test accuracy: 0.95434\n",
      "epoch:  20 | train accuracy: 0.96777 | test accuracy: 0.96296\n",
      "Federated learning for slice 4\n"
     ]
    }
   ],
   "source": [
    "# start_train_end_node_process()\n",
    "start_train_end_node_process_print_some(number_of_slices, print_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0459,  0.0528,  0.0031,  0.0325, -0.0223], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.0767, -0.0119,  0.0087,  0.0556, -0.0069], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "## As you can see, wieghts of local models are updated after training process\n",
    "print(main_model.fc2.weight[0,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare the performance of federated main model, individual local models and centralized model  \n",
    "\n",
    "**Federated main model vs individual local models before 1st iteration (on distributed test set)**  \n",
    "Since main model is randomly initialized and no action taken on it yet, its performance is very poor. Please before_acc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix for normal scenario for slices : 4\n",
      "[[4222   41 3024  414    4]\n",
      " [1107    2  363 3866    0]\n",
      " [   3    0    8    0    0]\n",
      " [ 148    0  238    2    0]\n",
      " [ 345   42  572  448    0]]\n",
      "confusion matrix for normal scenario for slices : 4\n",
      "[[7586   52    0    4   63]\n",
      " [  95 5242    0    0    1]\n",
      " [   8    0    0    3    0]\n",
      " [ 307   20    0   59    2]\n",
      " [  76    1    0    0 1330]]\n"
     ]
    }
   ],
   "source": [
    "before_acc_table=compare_local_and_merged_model_performance(number_of_slices=number_of_slices)\n",
    "before_test_loss, before_test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "file.write('\\nbefore training main model')\n",
    "confusion_mat(main_model, test_dl)\n",
    "\n",
    "main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_slices) \n",
    "\n",
    "after_acc_table=compare_local_and_merged_model_performance(number_of_slices=number_of_slices)\n",
    "after_test_loss, after_test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "file.write('\\nafter training main model')\n",
    "confusion_mat(main_model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated main model vs individual local models before FedAvg first iteration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>local_ind_model</th>\n",
       "      <th>merged_main_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample 0</td>\n",
       "      <td>0.9763</td>\n",
       "      <td>0.2851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample 1</td>\n",
       "      <td>0.9577</td>\n",
       "      <td>0.2851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample 2</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.2851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample 3</td>\n",
       "      <td>0.9608</td>\n",
       "      <td>0.2851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample  local_ind_model  merged_main_model\n",
       "0  sample 0           0.9763             0.2851\n",
       "1  sample 1           0.9577             0.2851\n",
       "2  sample 2           0.9630             0.2851\n",
       "3  sample 3           0.9608             0.2851"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Federated main model vs individual local models before FedAvg first iteration\")\n",
    "file.write('\\nBefore training federated')\n",
    "file.write('\\n'+str(before_acc_table))\n",
    "before_acc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated main model vs individual local models after FedAvg first iteration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>local_ind_model</th>\n",
       "      <th>merged_main_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample 0</td>\n",
       "      <td>0.9763</td>\n",
       "      <td>0.9574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample 1</td>\n",
       "      <td>0.9577</td>\n",
       "      <td>0.9574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample 2</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.9574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample 3</td>\n",
       "      <td>0.9608</td>\n",
       "      <td>0.9574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample  local_ind_model  merged_main_model\n",
       "0  sample 0           0.9763             0.9574\n",
       "1  sample 1           0.9577             0.9574\n",
       "2  sample 2           0.9630             0.9574\n",
       "3  sample 3           0.9608             0.9574"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Federated main model vs individual local models after FedAvg first iteration\")\n",
    "file.write('\\nAfter training federated')\n",
    "file.write('\\n'+str(after_acc_table))\n",
    "after_acc_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Federated main model vs centralized model before 1st iteration (on all test data)**  \n",
    "Please be aware that the centralized model gets approximately %98 accuracy on all test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before 1st iteration main model accuracy on all test data:  0.0661\n",
      "After 1st iteration main model accuracy on all test data:  0.9608\n",
      "Centralized model accuracy on all test data:  0.9536\n"
     ]
    }
   ],
   "source": [
    "print(\"Before 1st iteration main model accuracy on all test data: {:7.4f}\".format(before_test_accuracy))\n",
    "print(\"After 1st iteration main model accuracy on all test data: {:7.4f}\".format(after_test_accuracy))\n",
    "print(\"Centralized model accuracy on all test data: {:7.4f}\".format(central_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a single iteration, we can send the weights of the main model back to the nodes and repeat the above steps.\n",
    "Now let's check how the performance of the main model improves when we repeat the iteration 10 more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Iteration 2 : main_model accuracy on all test data:  0.9733\n",
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Iteration 3 : main_model accuracy on all test data:  0.9766\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_slices)\n",
    "    start_train_end_node_process_without_print(number_of_slices)\n",
    "    main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_slices) \n",
    "    test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "    print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix for normal scenario for slices : 4\n",
      "[[7629   53    0   16    7]\n",
      " [   9 5329    0    0    0]\n",
      " [   5    0    6    0    0]\n",
      " [ 183    3    1  199    2]\n",
      " [  59    0    0   10 1338]]\n"
     ]
    }
   ],
   "source": [
    "confusion_mat(main_model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Iteration 2 : main_model accuracy on all test data:  0.9754\n",
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Iteration 3 : main_model accuracy on all test data:  0.9778\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_slices)\n",
    "    start_train_end_node_process_without_print(number_of_slices)\n",
    "    main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_slices) \n",
    "    test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "    print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix for normal scenario for slices : 4\n",
      "[[7635   45    0   21    4]\n",
      " [   8 5330    0    0    0]\n",
      " [   5    0    6    0    0]\n",
      " [ 183    1    0  202    2]\n",
      " [  54    0    0    6 1347]]\n"
     ]
    }
   ],
   "source": [
    "file = open(file_name, \"a\")\n",
    "confusion_mat(main_model, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the centralized model was calculated as approximately 98%. The accuracy of the main model obtained by FedAvg method started from 85% and improved to 94%. In this case, we can say that although the main model obtained by FedAvg method was trained without seeing the data, its performance cannot be underestimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
